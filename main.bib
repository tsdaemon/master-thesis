% Encoding: UTF-8

@Unpublished{lstm-picture,
  author = {C. Olah},
  title  = {Understanding LSTM Networks},
  month  = aug,
  year   = {2015},
  url    = { http://colah.github.io/posts/2015-08-Understanding-LSTMs/},
}

@Misc{ling-484,
  author       = {A. C. Brett},
  title        = {Linguistics 484: Grammars},
  year         = {2003},
  url          = {http://web.uvic.ca/~ling48x/ling484/notes/},
}

@Article{Tai2015,
  author      = {Kai Sheng Tai and Richard Socher and Christopher D. Manning},
  title       = {Improved Semantic Representations From Tree-Structured Long Short-Term Memory Networks},
  abstract    = {Because of their superior ability to preserve sequence information over time, Long Short-Term Memory (LSTM) networks, a type of recurrent neural network with a more complex computational unit, have obtained strong results on a variety of sequence modeling tasks. The only underlying LSTM structure that has been explored so far is a linear chain. However, natural language exhibits syntactic properties that would naturally combine words to phrases. We introduce the Tree-LSTM, a generalization of LSTMs to tree-structured network topologies. Tree-LSTMs outperform all existing systems and strong LSTM baselines on two tasks: predicting the semantic relatedness of two sentences (SemEval 2014, Task 1) and sentiment classification (Stanford Sentiment Treebank).},
  date        = {2015-02-28},
  eprint      = {1503.00075v3},
  eprintclass = {cs.CL},
  eprinttype  = {arXiv},
  file        = {online:http\://arxiv.org/pdf/1503.00075v3:PDF},
  keywords    = {cs.CL, cs.AI, cs.LG},
}

@Article{Gers2001,
  author    = {F.A. Gers and E. Schmidhuber},
  title     = {{LSTM} recurrent networks learn simple context-free and context-sensitive languages},
  journal   = {{IEEE} Transactions on Neural Networks},
  year      = {2001},
  volume    = {12},
  number    = {6},
  pages     = {1333--1340},
  doi       = {10.1109/72.963769},
  publisher = {Institute of Electrical and Electronics Engineers ({IEEE})},
}

@Article{Barone2017,
  author      = {Antonio Valerio Miceli Barone and Rico Sennrich},
  title       = {A parallel corpus of Python functions and documentation strings for automated code documentation and code generation},
  abstract    = {Automated documentation of programming source code and automated code generation from natural language are challenging tasks of both practical and scientific interest. Progress in these areas has been limited by the low availability of parallel corpora of code and natural language descriptions, which tend to be small and constrained to specific domains. In this work we introduce a large and diverse parallel corpus of a hundred thousands Python functions with their documentation strings ("docstrings") generated by scraping open source repositories on GitHub. We describe baseline results for the code documentation and code generation tasks obtained by neural machine translation. We also experiment with data augmentation techniques to further increase the amount of training data. We release our datasets and processing scripts in order to stimulate research in these areas.},
  date        = {2017-07-07},
  eprint      = {1707.02275v1},
  eprintclass = {cs.CL},
  eprinttype  = {arXiv},
  file        = {online:http\://arxiv.org/pdf/1707.02275v1:PDF},
  keywords    = {cs.CL, cs.AI},
}

@InProceedings{quirk2015language,
  author    = {Quirk, Chris and Mooney, Raymond J and Galley, Michel},
  title     = {Language to Code: Learning Semantic Parsers for If-This-Then-That Recipes.},
  booktitle = {ACL (1)},
  year      = {2015},
  pages     = {878--888},
}

@Article{artzi2013weakly,
  author  = {Artzi, Yoav and Zettlemoyer, Luke},
  title   = {Weakly supervised learning of semantic parsers for mapping instructions to actions},
  journal = {Transactions of the Association for Computational Linguistics},
  year    = {2013},
  volume  = {1},
  pages   = {49--62},
}

@InProceedings{berant2013semantic,
  author    = {Berant, Jonathan and Chou, Andrew and Frostig, Roy and Liang, Percy},
  title     = {Semantic Parsing on Freebase from Question-Answer Pairs.},
  booktitle = {EMNLP},
  year      = {2013},
  volume    = {2},
  number    = {5},
  pages     = {6},
}

@Article{Zettlemoyer2012,
  author      = {Luke S. Zettlemoyer and Michael Collins},
  title       = {Learning to Map Sentences to Logical Form: Structured Classification with Probabilistic Categorial Grammars},
  abstract    = {This paper addresses the problem of mapping natural language sentences to lambda-calculus encodings of their meaning. We describe a learning algorithm that takes as input a training set of sentences labeled with expressions in the lambda calculus. The algorithm induces a grammar for the problem, along with a log-linear model that represents a distribution over syntactic and semantic analyses conditioned on the input sentence. We apply the method to the task of learning natural language interfaces to databases and show that the learned parsers outperform previous methods in two benchmark database domains.},
  date        = {2012-07-04},
  eprint      = {1207.1420v1},
  eprintclass = {cs.CL},
  eprinttype  = {arXiv},
  file        = {online:http\://arxiv.org/pdf/1207.1420v1:PDF},
  keywords    = {cs.CL},
}

@Article{Clark2007,
  author    = {Stephen Clark and James R. Curran},
  title     = {Wide-Coverage Efficient Statistical Parsing with {CCG} and Log-Linear Models},
  journal   = {Computational Linguistics},
  year      = {2007},
  volume    = {33},
  number    = {4},
  pages     = {493--552},
  month     = {dec},
  doi       = {10.1162/coli.2007.33.4.493},
  publisher = {{MIT} Press - Journals},
}

@InProceedings{banarescu2013abstract,
  author    = {Banarescu, Laura and Bonial, Claire and Cai, Shu and Georgescu, Madalina and Griffitt, Kira and Hermjakob, Ulf and Knight, Kevin and Koehn, Philipp and Palmer, Martha and Schneider, Nathan},
  title     = {Abstract meaning representation for sembanking},
  booktitle = {Proceedings of the 7th Linguistic Annotation Workshop and Interoperability with Discourse},
  year      = {2013},
  pages     = {178--186},
}

@Article{Chen2016,
  author      = {Xinyun Chen and Chang Liu and Richard Shin and Dawn Song and Mingcheng Chen},
  title       = {Latent Attention For If-Then Program Synthesis},
  abstract    = {Automatic translation from natural language descriptions into programs is a longstanding challenging problem. In this work, we consider a simple yet important sub-problem: translation from textual descriptions to If-Then programs. We devise a novel neural network architecture for this task which we train end-to-end. Specifically, we introduce Latent Attention, which computes multiplicative weights for the words in the description in a two-stage process with the goal of better leveraging the natural language structures that indicate the relevant parts for predicting program elements. Our architecture reduces the error rate by 28.57% compared to prior art. We also propose a one-shot learning scenario of If-Then program synthesis and simulate it with our existing dataset. We demonstrate a variation on the training procedure for this scenario that outperforms the original procedure, significantly closing the gap to the model trained with all data.},
  date        = {2016-11-07},
  eprint      = {1611.01867v1},
  eprintclass = {cs.CL},
  eprinttype  = {arXiv},
  file        = {online:http\://arxiv.org/pdf/1611.01867v1:PDF},
  keywords    = {cs.CL},
}

@Article{Ling2016,
  author      = {Wang Ling and Edward Grefenstette and Karl Moritz Hermann and Tomáš Kočiský and Andrew Senior and Fumin Wang and Phil Blunsom},
  title       = {Latent Predictor Networks for Code Generation},
  abstract    = {Many language generation tasks require the production of text conditioned on both structured and unstructured inputs. We present a novel neural network architecture which generates an output sequence conditioned on an arbitrary number of input functions. Crucially, our approach allows both the choice of conditioning context and the granularity of generation, for example characters or tokens, to be marginalised, thus permitting scalable and effective training. Using this framework, we address the problem of generating programming code from a mixed natural language and structured specification. We create two new data sets for this paradigm derived from the collectible trading card games Magic the Gathering and Hearthstone. On these, and a third preexisting corpus, we demonstrate that marginalising multiple predictors allows our model to outperform strong benchmarks.},
  date        = {2016-03-22},
  eprint      = {1603.06744v2},
  eprintclass = {cs.CL},
  eprinttype  = {arXiv},
  file        = {online:http\://arxiv.org/pdf/1603.06744v2:PDF},
  keywords    = {cs.CL, cs.NE},
}

@Article{Zhong2017,
  author      = {Victor Zhong and Caiming Xiong and Richard Socher},
  title       = {Seq2SQL: Generating Structured Queries from Natural Language using Reinforcement Learning},
  abstract    = {A significant amount of the world's knowledge is stored in relational databases. However, the ability for users to retrieve facts from a database is limited due to a lack of understanding of query languages such as SQL. We propose Seq2SQL, a deep neural network for translating natural language questions to corresponding SQL queries. Our model leverages the structure of SQL queries to significantly reduce the output space of generated queries. Moreover, we use rewards from in-the-loop query execution over the database to learn a policy to generate unordered parts of the query, which we show are less suitable for optimization via cross entropy loss. In addition, we will publish WikiSQL, a dataset of 80654 hand-annotated examples of questions and SQL queries distributed across 24241 tables from Wikipedia. This dataset is required to train our model and is an order of magnitude larger than comparable datasets. By applying policy-based reinforcement learning with a query execution environment to WikiSQL, our model Seq2SQL outperforms attentional sequence to sequence models, improving execution accuracy from 35.9% to 59.4% and logical form accuracy from 23.4% to 48.3%.},
  date        = {2017-08-31},
  eprint      = {1709.00103v4},
  eprintclass = {cs.CL},
  eprinttype  = {arXiv},
  file        = {online:http\://arxiv.org/pdf/1709.00103v4:PDF},
  keywords    = {cs.CL, cs.AI},
}

@Article{Rabinovich2017,
  author      = {Maxim Rabinovich and Mitchell Stern and Dan Klein},
  title       = {Abstract Syntax Networks for Code Generation and Semantic Parsing},
  abstract    = {Tasks like code generation and semantic parsing require mapping unstructured (or partially structured) inputs to well-formed, executable outputs. We introduce abstract syntax networks, a modeling framework for these problems. The outputs are represented as abstract syntax trees (ASTs) and constructed by a decoder with a dynamically-determined modular structure paralleling the structure of the output tree. On the benchmark Hearthstone dataset for code generation, our model obtains 79.2 BLEU and 22.7% exact match accuracy, compared to previous state-of-the-art values of 67.1 and 6.1%. Furthermore, we perform competitively on the Atis, Jobs, and Geo semantic parsing datasets with no task-specific engineering.},
  date        = {2017-04-25},
  eprint      = {1704.07535v1},
  eprintclass = {cs.CL},
  eprinttype  = {arXiv},
  file        = {online:http\://arxiv.org/pdf/1704.07535v1:PDF},
  keywords    = {cs.CL, cs.AI, cs.LG, stat.ML},
}

@InProceedings{socher2011parsing,
  author    = {Socher, Richard and Lin, Cliff C and Manning, Chris and Ng, Andrew Y},
  title     = {Parsing natural scenes and natural language with recursive neural networks},
  booktitle = {Proceedings of the 28th international conference on machine learning (ICML-11)},
  year      = {2011},
  pages     = {129--136},
}

@InProceedings{Goller,
  author        = {C. Goller and A. Kuchler},
  title         = {Learning task-dependent distributed representations by backpropagation through structure},
  booktitle     = {Proceedings of International Conference on Neural Networks ({ICNN}{\textquotesingle}96)},
  publisher     = {{IEEE}},
  __markedentry = {[Anatoliy:6]},
  doi           = {10.1109/icnn.1996.548916},
}

@Article{Dyer2016,
  author      = {Chris Dyer and Adhiguna Kuncoro and Miguel Ballesteros and Noah A. Smith},
  title       = {Recurrent Neural Network Grammars},
  abstract    = {We introduce recurrent neural network grammars, probabilistic models of sentences with explicit phrase structure. We explain efficient inference procedures that allow application to both parsing and language modeling. Experiments show that they provide better parsing in English than any single previously published supervised generative model and better language modeling than state-of-the-art sequential RNNs in English and Chinese.},
  date        = {2016-02-25},
  eprint      = {1602.07776v4},
  eprintclass = {cs.CL},
  eprinttype  = {arXiv},
  file        = {online:http\://arxiv.org/pdf/1602.07776v4:PDF},
  keywords    = {cs.CL, cs.NE},
}

@InProceedings{xie2017constituent,
  author    = {Xie, Pengtao and Xing, Eric},
  title     = {A Constituent-Centric Neural Architecture for Reading Comprehension},
  booktitle = {Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
  year      = {2017},
  volume    = {1},
  pages     = {1405--1414},
}

@Article{Chen2017,
  author      = {Huadong Chen and Shujian Huang and David Chiang and Jiajun Chen},
  title       = {Improved Neural Machine Translation with a Syntax-Aware Encoder and Decoder},
  abstract    = {Most neural machine translation (NMT) models are based on the sequential encoder-decoder framework, which makes no use of syntactic information. In this paper, we improve this model by explicitly incorporating source-side syntactic trees. More specifically, we propose (1) a bidirectional tree encoder which learns both sequential and tree structured representations; (2) a tree-coverage model that lets the attention depend on the source-side syntax. Experiments on Chinese-English translation demonstrate that our proposed models outperform the sequential attentional model as well as a stronger baseline with a bottom-up tree encoder and word coverage.},
  date        = {2017-07-18},
  eprint      = {1707.05436v1},
  eprintclass = {cs.CL},
  eprinttype  = {arXiv},
  file        = {online:http\://arxiv.org/pdf/1707.05436v1:PDF},
  keywords    = {cs.CL},
}

@InProceedings{Price2000,
  author    = {David Price and Ellen Rilofff and Joseph Zachary and Brandon Harvey},
  title     = {{NaturalJava}},
  booktitle = {Proceedings of the 5th international conference on Intelligent user interfaces - {IUI} {\textquotesingle}00},
  year      = {2000},
  publisher = {{ACM} Press},
  doi       = {10.1145/325737.325845},
}

@InProceedings{Srivastava2010,
  author    = {Saurabh Srivastava and Sumit Gulwani and Jeffrey S. Foster},
  title     = {From program verification to program synthesis},
  booktitle = {Proceedings of the 37th annual {ACM} {SIGPLAN}-{SIGACT} symposium on Principles of programming languages - {POPL} {\textquotesingle}10},
  year      = {2010},
  publisher = {{ACM} Press},
  doi       = {10.1145/1706299.1706337},
}

@Book{haugeland1989artificial,
  title     = {Artificial intelligence: The very idea},
  publisher = {MIT press},
  year      = {1989},
  author    = {Haugeland, John},
}

@Article{Miriyala1991,
  author    = {K. Miriyala and M.T. Harandi},
  title     = {Automatic derivation of formal software specifications from informal descriptions},
  journal   = {{IEEE} Transactions on Software Engineering},
  year      = {1991},
  volume    = {17},
  number    = {10},
  pages     = {1126--1142},
  doi       = {10.1109/32.99198},
  publisher = {Institute of Electrical and Electronics Engineers ({IEEE})},
}

@Article{Bahdanau2014,
  author      = {Dzmitry Bahdanau and Kyunghyun Cho and Yoshua Bengio},
  title       = {Neural Machine Translation by Jointly Learning to Align and Translate},
  abstract    = {Neural machine translation is a recently proposed approach to machine translation. Unlike the traditional statistical machine translation, the neural machine translation aims at building a single neural network that can be jointly tuned to maximize the translation performance. The models proposed recently for neural machine translation often belong to a family of encoder-decoders and consists of an encoder that encodes a source sentence into a fixed-length vector from which a decoder generates a translation. In this paper, we conjecture that the use of a fixed-length vector is a bottleneck in improving the performance of this basic encoder-decoder architecture, and propose to extend this by allowing a model to automatically (soft-)search for parts of a source sentence that are relevant to predicting a target word, without having to form these parts as a hard segment explicitly. With this new approach, we achieve a translation performance comparable to the existing state-of-the-art phrase-based system on the task of English-to-French translation. Furthermore, qualitative analysis reveals that the (soft-)alignments found by the model agree well with our intuition.},
  date        = {2014-09-01},
  eprint      = {1409.0473v7},
  eprintclass = {cs.CL},
  eprinttype  = {arXiv},
  file        = {online:http\://arxiv.org/pdf/1409.0473v7:PDF},
  keywords    = {cs.CL, cs.LG, cs.NE, stat.ML},
}

@InCollection{NIPS2014_5346,
  author    = {Sutskever, Ilya and Vinyals, Oriol and Le, Quoc V},
  title     = {Sequence to Sequence Learning with Neural Networks},
  booktitle = {Advances in Neural Information Processing Systems 27},
  publisher = {Curran Associates, Inc.},
  year      = {2014},
  editor    = {Z. Ghahramani and M. Welling and C. Cortes and N. D. Lawrence and K. Q. Weinberger},
  pages     = {3104--3112},
  url       = {http://papers.nips.cc/paper/5346-sequence-to-sequence-learning-with-neural-networks.pdf},
}

@Article{Jean2014,
  author      = {Sébastien Jean and Kyunghyun Cho and Roland Memisevic and Yoshua Bengio},
  title       = {On Using Very Large Target Vocabulary for Neural Machine Translation},
  abstract    = {Neural machine translation, a recently proposed approach to machine translation based purely on neural networks, has shown promising results compared to the existing approaches such as phrase-based statistical machine translation. Despite its recent success, neural machine translation has its limitation in handling a larger vocabulary, as training complexity as well as decoding complexity increase proportionally to the number of target words. In this paper, we propose a method that allows us to use a very large target vocabulary without increasing training complexity, based on importance sampling. We show that decoding can be efficiently done even with the model having a very large target vocabulary by selecting only a small subset of the whole target vocabulary. The models trained by the proposed approach are empirically found to outperform the baseline models with a small vocabulary as well as the LSTM-based neural machine translation models. Furthermore, when we use the ensemble of a few models with very large target vocabularies, we achieve the state-of-the-art translation performance (measured by BLEU) on the English->German translation and almost as high performance as state-of-the-art English->French translation system.},
  date        = {2014-12-05},
  eprint      = {1412.2007v2},
  eprintclass = {cs.CL},
  eprinttype  = {arXiv},
  file        = {online:http\://arxiv.org/pdf/1412.2007v2:PDF},
  keywords    = {cs.CL},
}

@Article{harnad1990symbol,
  author    = {Harnad, Stevan},
  title     = {The symbol grounding problem},
  journal   = {Physica D: Nonlinear Phenomena},
  year      = {1990},
  volume    = {42},
  number    = {1-3},
  pages     = {335--346},
  publisher = {Elsevier},
}

@Article{Luong2015,
  author      = {Minh-Thang Luong and Hieu Pham and Christopher D. Manning},
  title       = {Effective Approaches to Attention-based Neural Machine Translation},
  abstract    = {An attentional mechanism has lately been used to improve neural machine translation (NMT) by selectively focusing on parts of the source sentence during translation. However, there has been little work exploring useful architectures for attention-based NMT. This paper examines two simple and effective classes of attentional mechanism: a global approach which always attends to all source words and a local one that only looks at a subset of source words at a time. We demonstrate the effectiveness of both approaches over the WMT translation tasks between English and German in both directions. With local attention, we achieve a significant gain of 5.0 BLEU points over non-attentional systems which already incorporate known techniques such as dropout. Our ensemble model using different attention architectures has established a new state-of-the-art result in the WMT'15 English to German translation task with 25.9 BLEU points, an improvement of 1.0 BLEU points over the existing best system backed by NMT and an n-gram reranker.},
  date        = {2015-08-17},
  eprint      = {1508.04025v5},
  eprintclass = {cs.CL},
  eprinttype  = {arXiv},
  file        = {online:http\://arxiv.org/pdf/1508.04025v5:PDF},
  keywords    = {cs.CL},
}

@Article{Wu2016,
  author      = {Yonghui Wu and Mike Schuster and Zhifeng Chen and Quoc V. Le and Mohammad Norouzi and Wolfgang Macherey and Maxim Krikun and Yuan Cao and Qin Gao and Klaus Macherey and Jeff Klingner and Apurva Shah and Melvin Johnson and Xiaobing Liu and Łukasz Kaiser and Stephan Gouws and Yoshikiyo Kato and Taku Kudo and Hideto Kazawa and Keith Stevens and George Kurian and Nishant Patil and Wei Wang and Cliff Young and Jason Smith and Jason Riesa and Alex Rudnick and Oriol Vinyals and Greg Corrado and Macduff Hughes and Jeffrey Dean},
  title       = {Google's Neural Machine Translation System: Bridging the Gap between Human and Machine Translation},
  abstract    = {Neural Machine Translation (NMT) is an end-to-end learning approach for automated translation, with the potential to overcome many of the weaknesses of conventional phrase-based translation systems. Unfortunately, NMT systems are known to be computationally expensive both in training and in translation inference. Also, most NMT systems have difficulty with rare words. These issues have hindered NMT's use in practical deployments and services, where both accuracy and speed are essential. In this work, we present GNMT, Google's Neural Machine Translation system, which attempts to address many of these issues. Our model consists of a deep LSTM network with 8 encoder and 8 decoder layers using attention and residual connections. To improve parallelism and therefore decrease training time, our attention mechanism connects the bottom layer of the decoder to the top layer of the encoder. To accelerate the final translation speed, we employ low-precision arithmetic during inference computations. To improve handling of rare words, we divide words into a limited set of common sub-word units ("wordpieces") for both input and output. This method provides a good balance between the flexibility of "character"-delimited models and the efficiency of "word"-delimited models, naturally handles translation of rare words, and ultimately improves the overall accuracy of the system. Our beam search technique employs a length-normalization procedure and uses a coverage penalty, which encourages generation of an output sentence that is most likely to cover all the words in the source sentence. On the WMT'14 English-to-French and English-to-German benchmarks, GNMT achieves competitive results to state-of-the-art. Using a human side-by-side evaluation on a set of isolated simple sentences, it reduces translation errors by an average of 60% compared to Google's phrase-based production system.},
  date        = {2016-09-26},
  eprint      = {1609.08144v2},
  eprintclass = {cs.CL},
  eprinttype  = {arXiv},
  file        = {online:http\://arxiv.org/pdf/1609.08144v2:PDF},
  keywords    = {cs.CL, cs.AI, cs.LG},
}

@Article{mcdermott1987critique,
  author    = {McDermott, Drew},
  title     = {A critique of pure reason},
  journal   = {Computational intelligence},
  year      = {1987},
  volume    = {3},
  number    = {1},
  pages     = {151--160},
  publisher = {Wiley Online Library},
}

@Book{goodfellow2016deep,
  title     = {Deep learning},
  publisher = {MIT press},
  year      = {2016},
  author    = {Goodfellow, Ian and Bengio, Yoshua and Courville, Aaron},
}

@InProceedings{Treude2011,
  author    = {Christoph Treude and Ohad Barzilay and Margaret-Anne Storey},
  title     = {How do programmers ask and answer questions on the web?},
  booktitle = {Proceeding of the 33rd international conference on Software engineering - {ICSE} {\textquotesingle}11},
  year      = {2011},
  publisher = {{ACM} Press},
  doi       = {10.1145/1985793.1985907},
}

@Article{Jozefowicz2016,
  author      = {Rafal Jozefowicz and Oriol Vinyals and Mike Schuster and Noam Shazeer and Yonghui Wu},
  title       = {Exploring the Limits of Language Modeling},
  abstract    = {In this work we explore recent advances in Recurrent Neural Networks for large scale Language Modeling, a task central to language understanding. We extend current models to deal with two key challenges present in this task: corpora and vocabulary sizes, and complex, long term structure of language. We perform an exhaustive study on techniques such as character Convolutional Neural Networks or Long-Short Term Memory, on the One Billion Word Benchmark. Our best single model significantly improves state-of-the-art perplexity from 51.3 down to 30.0 (whilst reducing the number of parameters by a factor of 20), while an ensemble of models sets a new record by improving perplexity from 41.0 down to 23.7. We also release these models for the NLP and ML community to study and improve upon.},
  date        = {2016-02-07},
  eprint      = {1602.02410v2},
  eprintclass = {cs.CL},
  eprinttype  = {arXiv},
  file        = {online:http\://arxiv.org/pdf/1602.02410v2:PDF},
  keywords    = {cs.CL},
}

@Article{hochreiter1997long,
  author    = {Hochreiter, Sepp and Schmidhuber, J{\"u}rgen},
  title     = {Long short-term memory},
  journal   = {Neural computation},
  year      = {1997},
  volume    = {9},
  number    = {8},
  pages     = {1735--1780},
  publisher = {MIT Press},
}

@InProceedings{sundermeyer2012lstm,
  author    = {Sundermeyer, Martin and Schl{\"u}ter, Ralf and Ney, Hermann},
  title     = {LSTM neural networks for language modeling},
  booktitle = {Thirteenth Annual Conference of the International Speech Communication Association},
  year      = {2012},
}

@Article{bengio2003neural,
  author  = {Bengio, Yoshua and Ducharme, R{\'e}jean and Vincent, Pascal and Jauvin, Christian},
  title   = {A neural probabilistic language model},
  journal = {Journal of machine learning research},
  year    = {2003},
  volume  = {3},
  number  = {Feb},
  pages   = {1137--1155},
}

@Book{white1992artificial,
  title     = {Artificial neural networks: approximation and learning theory},
  publisher = {Blackwell Publishers, Inc.},
  year      = {1992},
  author    = {White, Halbert},
}

@Article{dreyfus1994computers,
  author  = {Dreyfus, Hubert L},
  title   = {What computers still can't do},
  journal = {Topics in Health Information Management},
  year    = {1994},
  volume  = {15},
  number  = {1},
  pages   = {87},
}

@Article{balzer1978informality,
  author    = {Balzer, Robert and Goldman, Noreen and Wile, David},
  title     = {Informality in program specifications},
  journal   = {IEEE Transactions on Software Engineering},
  year      = {1978},
  number    = {2},
  pages     = {94--103},
  publisher = {IEEE},
}

@Article{little2009keyword,
  author    = {Little, Greg and Miller, Robert C},
  title     = {Keyword programming in Java},
  journal   = {Automated Software Engineering},
  year      = {2009},
  volume    = {16},
  number    = {1},
  pages     = {37},
  publisher = {Springer},
}

@Article{barstow1979experiment,
  author    = {Barstow, David R},
  title     = {An experiment in knowledge-based automatic programming},
  journal   = {Artificial Intelligence},
  year      = {1979},
  volume    = {12},
  number    = {2},
  pages     = {73--119},
  publisher = {Elsevier},
}

@InProceedings{Solar-Lezama2005,
  author    = {Armando Solar-Lezama and Rodric Rabbah and Rastislav Bod{\'{\i}}k and Kemal Ebcio{\u{g}}lu},
  title     = {Programming by sketching for bit-streaming programs},
  booktitle = {Proceedings of the 2005 {ACM} {SIGPLAN} conference on Programming language design and implementation - {PLDI} {\textquotesingle}05},
  year      = {2005},
  publisher = {{ACM} Press},
  doi       = {10.1145/1065010.1065045},
}

@InProceedings{green1977summary,
  author    = {Green, Cordell and others},
  title     = {A Summary of the PSI Program Synthesis System.},
  booktitle = {IJCAI},
  year      = {1977},
  volume    = {5},
  pages     = {380--381},
}

@InProceedings{Jha2010,
  author    = {Susmit Jha and Sumit Gulwani and Sanjit A. Seshia and Ashish Tiwari},
  title     = {Oracle-guided component-based program synthesis},
  booktitle = {Proceedings of the 32nd {ACM}/{IEEE} International Conference on Software Engineering - {ICSE} {\textquotesingle}10},
  year      = {2010},
  publisher = {{ACM} Press},
  doi       = {10.1145/1806799.1806833},
}

@InProceedings{green1976design,
  author       = {Green, Cordell},
  title        = {The design of the PSI program synthesis system},
  booktitle    = {Proceedings of the 2nd international conference on Software engineering},
  year         = {1976},
  pages        = {4--18},
  organization = {IEEE Computer Society Press},
}

@InProceedings{Brandt2010,
  author    = {Joel Brandt and Mira Dontcheva and Marcos Weskamp and Scott R. Klemmer},
  title     = {Example-centric programming},
  booktitle = {Proceedings of the 28th international conference on Human factors in computing systems - {CHI} {\textquotesingle}10},
  year      = {2010},
  publisher = {{ACM} Press},
  doi       = {10.1145/1753326.1753402},
}

@Article{Robillard1999,
  author    = {Pierre N. Robillard},
  title     = {The role of knowledge in software development},
  journal   = {Communications of the {ACM}},
  year      = {1999},
  volume    = {42},
  number    = {1},
  pages     = {87--92},
  month     = {jan},
  doi       = {10.1145/291469.291476},
  publisher = {Association for Computing Machinery ({ACM})},
}

@InProceedings{Brandt2009,
  author    = {Joel Brandt and Philip J. Guo and Joel Lewenstein and Mira Dontcheva and Scott R. Klemmer},
  title     = {Two studies of opportunistic programming},
  booktitle = {Proceedings of the 27th international conference on Human factors in computing systems - {CHI} 09},
  year      = {2009},
  publisher = {{ACM} Press},
  doi       = {10.1145/1518701.1518944},
}

@TechReport{green1969application,
  author      = {Green, Cordell},
  title       = {Application of theorem proving to problem solving},
  institution = {SRI INTERNATIONAL MENLO PARK CA ARTIFICIAL INTELLIGENCE CENTER},
  year        = {1969},
}

@Article{Lee1974,
  author    = {R. C. T. Lee and R. J. Waldinger and C. L. Chang},
  title     = {An improved program-synthesizing algorithm and its correctness},
  journal   = {Communications of the {ACM}},
  year      = {1974},
  volume    = {17},
  number    = {4},
  pages     = {211--217},
  month     = {apr},
  doi       = {10.1145/360924.360967},
  publisher = {Association for Computing Machinery ({ACM})},
}

@Article{Balzer1985,
  author    = {R. Balzer},
  title     = {A 15 Year Perspective on Automatic Programming},
  journal   = {{IEEE} Transactions on Software Engineering},
  year      = {1985},
  volume    = {{SE}-11},
  number    = {11},
  pages     = {1257--1268},
  month     = {nov},
  doi       = {10.1109/tse.1985.231877},
  publisher = {Institute of Electrical and Electronics Engineers ({IEEE})},
}

@Article{Raychev2014,
  author    = {Veselin Raychev and Martin Vechev and Eran Yahav},
  title     = {Code completion with statistical language models},
  journal   = {{ACM} {SIGPLAN} Notices},
  year      = {2014},
  volume    = {49},
  number    = {6},
  pages     = {419--428},
  month     = {jun},
  doi       = {10.1145/2666356.2594321},
  publisher = {Association for Computing Machinery ({ACM})},
}

@InProceedings{Gvero2015,
  author    = {Gvero, Tihomir and Kuncak, Viktor},
  title     = {Interactive Synthesis Using Free-form Queries},
  booktitle = {Proceedings of the 37th International Conference on Software Engineering - Volume 2},
  year      = {2015},
  series    = {ICSE '15},
  pages     = {689--692},
  address   = {Piscataway, NJ, USA},
  publisher = {IEEE Press},
  acmid     = {2819139},
  location  = {Florence, Italy},
  numpages  = {4},
  url       = {http://dl.acm.org/citation.cfm?id=2819009.2819139},
}

@InProceedings{backus1957fortran,
  author       = {Backus, John W and Beeber, Robert J and Best, Sheldon and Goldberg, Richard and Haibt, L Mitchell and Herrick, Harlan L and Nelson, Robert A and Sayre, David and Sheridan, Peter B and Stern, H and others},
  title        = {The FORTRAN automatic coding system},
  booktitle    = {Papers presented at the February 26-28, 1957, western joint computer conference: Techniques for reliability},
  year         = {1957},
  pages        = {188--198},
  organization = {ACM},
}

@InProceedings{Galenson2014,
  author    = {Joel Galenson and Philip Reames and Rastislav Bodik and Björn Hartmann and Koushik Sen},
  title     = {{CodeHint}: dynamic and interactive synthesis of code snippets},
  booktitle = {Proceedings of the 36th International Conference on Software Engineering - {ICSE} 2014},
  year      = {2014},
  publisher = {{ACM} Press},
  doi       = {10.1145/2568225.2568250},
}

@Article{Yin2017,
  author      = {Pengcheng Yin and Graham Neubig},
  title       = {A Syntactic Neural Model for General-Purpose Code Generation},
  abstract    = {We consider the problem of parsing natural language descriptions into source code written in a general-purpose programming language like Python. Existing data-driven methods treat this problem as a language generation task without considering the underlying syntax of the target programming language. Informed by previous work in semantic parsing, in this paper we propose a novel neural architecture powered by a grammar model to explicitly capture the target syntax as prior knowledge. Experiments find this an effective way to scale up to generation of complex programs from natural language descriptions, achieving state-of-the-art results that well outperform previous code generation and semantic parsing approaches.},
  date        = {2017-04-06},
  eprint      = {1704.01696v1},
  eprintclass = {cs.CL},
  eprinttype  = {arXiv},
  file        = {online:http\://arxiv.org/pdf/1704.01696v1:PDF},
  keywords    = {cs.CL, cs.PL, cs.SE},
}

@Article{Zhu2015,
  author      = {Chenxi Zhu and Xipeng Qiu and Xinchi Chen and Xuanjing Huang},
  title       = {A Re-ranking Model for Dependency Parser with Recursive Convolutional Neural Network},
  abstract    = {In this work, we address the problem to model all the nodes (words or phrases) in a dependency tree with the dense representations. We propose a recursive convolutional neural network (RCNN) architecture to capture syntactic and compositional-semantic representations of phrases and words in a dependency tree. Different with the original recursive neural network, we introduce the convolution and pooling layers, which can model a variety of compositions by the feature maps and choose the most informative compositions by the pooling layers. Based on RCNN, we use a discriminative model to re-rank a $k$-best list of candidate dependency parsing trees. The experiments show that RCNN is very effective to improve the state-of-the-art dependency parsing on both English and Chinese datasets.},
  date        = {2015-05-21},
  eprint      = {1505.05667v1},
  eprintclass = {cs.CL},
  eprinttype  = {arXiv},
  file        = {online:http\://arxiv.org/pdf/1505.05667v1:PDF},
  keywords    = {cs.CL, cs.LG, cs.NE},
}

@InProceedings{Oda2015,
  author    = {Yusuke Oda and Hiroyuki Fudaba and Graham Neubig and Hideaki Hata and Sakriani Sakti and Tomoki Toda and Satoshi Nakamura},
  title     = {Learning to Generate Pseudo-Code from Source Code Using Statistical Machine Translation (T)},
  booktitle = {2015 30th {IEEE}/{ACM} International Conference on Automated Software Engineering ({ASE})},
  year      = {2015},
  month     = {nov},
  publisher = {{IEEE}},
  doi       = {10.1109/ase.2015.36},
}

@Comment{jabref-meta: databaseType:bibtex;}
