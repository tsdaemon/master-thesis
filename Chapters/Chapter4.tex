\chapter{Model architecture} 
\label{Chapter4}

\section{Code generation problem}
Given a natural language description $\Lambda$ our task is to infer the Python code $C$ based on the intent of the $\Lambda$. This task is solved by a generation of an underlying abstract syntax tree $\Tau$. $\Tau$ can be deterministically converted to a Python code $C$, though in this work source code $C$ and its abstract syntax tree are considered equivalent. A probabilistic grammar model of generating an abstract syntax tree $\Tau$ given $\Lambda$ is defined as $p(\Tau|\Lambda)$. The best corresponding syntax tree $\Tau$ is defined as
\begin{equation}
\Tau=\underset{\Tau}{\operatorname{argmax}}\, p(\Tau|\Lambda)
\label{eqn:main_problem}
\end{equation}
Probability \ref{eqn:main_problem} is modeled with neural model with set of weights $\theta$. To learn value of $\theta$ I use set of training examples, which consist of tuples $(\Tau^{\Lambda}, \Lambda)$. The parameters of the model are learnt by maximizing the conditional log-probabilities for the training set:
\begin{equation}
\theta=\underset{\theta}{\operatorname{argmax}}\, \sum{\Tau^{\Lambda}, \Lambda} log p(\Tau^{\Lambda}|\Lambda; \theta)
\label{eqn:mle}
\end{equation}
\section{Abstract syntax tree decoding}
The output of decoder is abstract syntax tree which consist of terminal and non-terminal nodes (\ref{fig:ast}). As suggested in \cite{Yin2017}, I factorize the generation process of AST into sequential application of actions of two types:
\begin{itemize}
	\item \code{ApplyRule[r]} corresponds to non-terminal nodes. It applies a production rule \code{r} to the current derivation tree.
	\item \code{GetToken[v]} correspond to terminal nodes. It finishes the node by appending a token \code{v}.
\end{itemize}

%
% Add generation process example for fig:ast
%

Under this grammar model, the probability of generating an AST $\Tau$ is factorized to rules $a$ as:
\begin{equation}
p(\Tau|\Lambda) = \prod{N}{n=1} p(a_n|\Lambda, a_{<n})
\label{eqn:factorized}
\end{equation}

\section{Encoder}
% A* CCG Parsing with a Supertag and Dependency Factored Model
% https://github.com/dasguptar/treelstm.pytorch
% 10.5281/zenodo.1012577 - tqdm
% https://nlp.stanford.edu/pubs/glove.pdf 
% https://github.com/dasguptar/treelstm.pytorch
% https://web.stanford.edu/~jurafsky/slp3/
% https://nlp.stanford.edu/software/stanford-dependencies.shtml
% Danqi Chen and Christopher D Manning. 2014. A Fast and Accurate Dependency Parser using Neural Networks. Proceedings of EMNLP 2014
% Jonathan Berant, Andrew Chou, Roy Frostig, Percy Liang. Semantic Parsing on Freebase from Question-Answer Pairs. Empirical Methods in Natural Language Processing (EMNLP), 2013.
% CCG - mark steedman


\section{Attention}

\section{Learning}
Beam search.