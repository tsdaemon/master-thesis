\chapter{Model} 
\label{Chapter4}

\section{Code generation problem}
Given a natural language description $x$ our task is to infer the Python code $y$ based on the intent of the $x$. Python code $y$ can be deterministically converted to an AST $\tau$ and vice-versa, though in this work a source code $y$ and its abstract syntax tree $\tau$ are considered equivalent. A probabilistic grammar model of generating an abstract syntax tree $\tau$ given description $x$ is defined as $P(\tau|x)$. The best corresponding syntax tree $\tau$ is defined as
\begin{equation}
\hat{\tau}=\underset{\tau}{\operatorname{argmax}}\: p(\tau|x)
\label{eqn:main_problem}
\end{equation}
Probability from Eq. \ref{eqn:main_problem} is modeled with neural model with a set of weights $\theta$. To learn values of $\theta$ we used a set of training examples, which consist of tuples $(\tau^{(x)}, x)$. The parameters of the model are learned by maximizing the conditional log-probabilities for the training set:
\begin{equation}
\theta=\underset{\theta}{\operatorname{argmax}} \sum_{\tau^{(x)}, x} log \: p(\tau^{(x)}|x; \theta)
\label{eqn:mle}
\end{equation}

\section{Abstract syntax tree generation} \label{ast_gen}
The output of decoder is an abstract syntax tree which consist of terminal and non-terminal nodes. As suggested in work of \cite{Yin2017}, we factorized the generation process of AST into sequential application of actions of two types:
\begin{itemize}
	\item \code{ApplyRule[r]} corresponds to non-terminal nodes. It applies a production rule \code{r} to the current derivation tree.
	\item \code{GenToken[v]} corresponds to terminal nodes. It finishes the node by appending a token \code{v}.
\end{itemize}

Let us consider as example the AST from \cref{fig:ast}. Its generation consist of the following actions:

\begin{codelist}{AST production sequence.}
\begin{verbnobox}[\verbarg]
ApplyRule[root => (stmt*)]
    ApplyRule[stmt => (Assign)]
        ApplyRule[Assign => (expr*{targets}), (expr{value})]
            ApplyRule[expr* => (expr)]
                ApplyRule[expr => (Name)]
                    ApplyRule[Name -> (str{id})]
                        GenToken["ls"]
                        GenToken["<eos>"]
            ApplyRule[expr => (Call)]
                ApplyRule[Call => (expr{func}), (expr*{args}), (keyword*{keywords})]
                    ApplyRule[expr => (Name)]
                        ApplyRule[Name -> (str{id})]
                            GenToken["sorted"]
                            GenToken["<eos>"]
                    ApplyRule[expr* => expr]
                        ApplyRule[expr => (Name)]
                            ApplyRule[Name => (str{id})]
                                GenToken["a"]
                                GenToken["<eos>"]
                    ApplyRule[keyword* => keyword]
                        ApplyRule[keyword => (str{arg}), (expr{value})]
                            GenToken["key"]
                            GenToken["<eos>"]
                            ApplyRule[expr => (Lambda)]
...
ApplyRule[Lambda => (arguments args), (expr body)]
    ApplyRule[arguments => (arg* args)]
        ApplyRule[arg* => arg]
            ApplyRule[arg => (str{arg})]
                GenToken["x"]
                GenToken["<eos>"]
    ApplyRule[expr => Subscript]
        ApplyRule[Subscript => (expr{value}), (slice{slice})]
            ApplyRule[expr => (Name)]
                ApplyRule[Name => (str{id})]
                    GenToken["x"]
                    GenToken["<eos>"]
            ApplyRule[slice => (Index)]
                ApplyRule[Index => (expr{value})]
                    ApplyRule[expr => (Num)]
                        ApplyRule[Num => (int{n})]
                            GenToken["1"]
                            GenToken["<eos>"]
\end{verbnobox}
\label{code:ast_production}
\end{codelist}

Under this grammar model, the probability of generating an AST $\tau$ is factorized as:
\begin{equation}
p(\tau|x) = \prod^{n}_{t=1} p(a^{(t)}|x, a^{(<t)})
\label{eqn:tree_probability}
\end{equation}
where $a^{t}$ is an action taken at the time step $t$ and $a_{<t}$ is the sequence of actions before $t$. 

For each time step $t$ the model selects the next action with a maximum probability --- \code{ApplyRule} to grow the tree or \code{GenToken} to fill values in terminal nodes. We must notice, that this model have one important flaw. List nodes like a \code{expr*} can not be expanded to an arbitrary number of children. Each number of children require separate rule in the grammar, like \code{expr* => (expr), (expr)}, \code{expr* => (expr), (expr), (expr)} etc. Therefore this model will require huge vocabulary to cover all possible production rules in dataset with arbitrary length functions. We proposed our solution to this issue in \cref{improve}.

\subsection{ApplyRule actions}
At any generation moment a tree $\tau$ contains a single frontier node (for time step $t$ it is $n_{f_t}$). An action \code{ApplyRule} expands frontier node in depth-first, left-to-right traversal of the tree. A production rule $r$ expands $n_{f_t}$ by appending all child nodes specified by the selected production. For example, in listing \ref{code:ast_production} in step 10 rule for node \code{Call} extends this node with three new nodes: \code{expr\{func\}}, \code{expr*\{args\}}, \code{keyword*\{keywords\}}. 

When $n_{f_t}$ is a terminal node, which can not be expanded further, the next action must be \code{GenToken}.

\textbf{Unary closures}. Sometimes, generating an AST requires applying a chain of unary productions. For example, in listing \ref{code:ast_production} in steps 4-6 it takes three time step to generate target for \code{Assign} statement:

\begin{verbatim}
ApplyRule[expr* => (expr)]
    ApplyRule[expr => (Name)]
        ApplyRule[Name -> (str{id})]
\end{verbatim}

Such a formal redundancy allows to have smaller production rule grammar but would increase a sequence length. Thus, they can be replaced with one action by taking the closure of the chain of unary productions:

\begin{verbatim}
ApplyRule[expr* => (str{id})]
\end{verbatim}

Model was tested both with and without unary closures.

\subsection{GenToken actions} \label{gentoken}
If a tree has reached a leaf and $n_{f_t}$ is a terminal node, the \code{GenToken} actions is used to fill this node. Each token generation ends with special end-of-string token \code{"<eos>"}. This way complex tokens like the function name \code{sortBySecondIndex} can be split on parts \code{['sort', 'By', 'Second', 'Index']}, thus reduce the token vocabulary and allow a complex rare token to be constructed from its constituents. After the end-of-string token generation model proceeds to the next frontier node.

The vocabulary of predefined token values can be inferred from the dataset. However, it is clear that this vocabulary will not cover all possible tokens for any environment. To cope with this problem, values can be copied directly from the input sequence. Therefore, it allows the model to use literals and names from a code description.

\section{Action probabilities}
Probabilities in \cref{eqn:tree_probability} are estimated by a neural attentional encoder-decoder model. Both encoder and decoder informational flow is structured by syntactic trees. 

\subsection{Encoder}
The main architectural novelty in this work is a Tree-LSTM encoder. Details about its input structures and implementation described below.

Input NL description $x$ consist of two parts. The first is a sequence of length $n$ of word vectors $\{w^{(t)}\}^n_{t=1}$. The second is a syntax tree which consists of $m$ nodes $\{\eta^{(t)}\}^m_{t=1}$, where $m\geq n$. Details about the description tree parsing can be found in \cref{preprocessing}. For all $t \leq n$, each tree node $\eta^{(t)}$ has a corresponding input vector from a word sequences $w^{(t)}$. For $t > n$, an input vector for $\eta^{(t)}$ is padding-vector. Each tree node $\eta^{(t)}$ has a set of children nodes $ch(\eta^{(t)})=\{\eta_{i}\}^k_{i=1}$. Therefore a single input element $x^{(t)}$ can be defined as a tuple $(\eta^{(t)}, w^{(t)}, ch(\eta^{(t)}))$:

\begin{equation}
 x^{(t)} =
  \begin{cases}
    (\eta^{(t)}, w^{(t)}, ch(\eta^{(t)}))  & \quad \text{if } t\leq n\\
    (\eta^{(t)}, w_{pad}, ch(\eta^{(t)}))  & \quad \text{if } t > n
  \end{cases}
\label{eq:enc_input}
\end{equation}

To encode this structures we used Tree-LSTM from \cite{Tai2015}\footnote{We used pytorch implementation from \href{https://github.com/dasguptar/treelstm.pytorch}{https://github.com/dasguptar/treelstm.pytorch}}. Similary to SRvN, described in \cref{sec:rvnn}, this model starts from tree leaves, and recursively computes a node embedding $h^{(t)}$ for each $x^{(t)}$ using values of a memory cell $\{c_i\}^{k}_{i=1} = memory(ch(\eta^{(t)}))$ and previous  embeddings $\{h_i\}^{k}_{i=1} = hidden(ch(\eta^{(t)}))$ from children nodes:

\begin{equation}
\begin{gathered}
    \hat{h} = \sum^{k}_{i=1}h_i \\
    
    i^{(t)} = \sigma(W_i\cdot[\hat{h}, w^{(t)}]+b_i) \\
    
    u^{(t)} = tanh(W_u\cdot[\hat{h}, w^{(t)}]+b_u) \\
    
    f^{(t)}_i = \sigma(W_{f}\cdot [h_i, w^{(t)}] + b_f) \\
    
    c^{(t)} = i^{(t)} \circ u^{(t)} + \sum_{i=1}^{k} f^{(t)}_i \circ c_i \\
    
    o^{(t)} = \sigma(W_o\cdot[\hat{h}, w^{(t)}]+b_o) \\
    
    h^{(t)} = o^{(t)} \circ tanh(c^{(t)})

\end{gathered}
\label{eq:tree_lstm}
\end{equation}

\subsection{Decoder}
The decoder is a RNN which sequentially generates an AST model as defined in \cref{eqn:tree_probability}. Each production action naturally grounds to a step in the decoder. This way, the sequence of production rules from listing \ref{code:ast_production} can be interpreted as unrolling RNN time steps with some additional connections from parent action steps.

We used implementation of decoder from \cite{Yin2017}. It is vanilla LSTM with additional connections which reflect the topological structure of the code syntax. For each decoding step, input vector is concatenation of a frontier node embedding $n^{(t)}$, a previous action embedding $a^{(t-1)}$ and a parent feeding $p^{(t)}$. The parent feeding is a concatenation of a decoder hidden state from parent step $h_{dp}^{(t)}$ and a parent rule embedding $r_p^{(t)}$. Consider as example step 9 in listing \ref{code:ast_production}: 
    
\begin{verbatim}
ApplyRule[Assign => (expr*{targets}), (expr{value})]
    ...............
                GetToken["<eos>"]
    ApplyRule[expr => (Call)]
\end{verbatim}

It has a frontier node \code{expr}, a previous action \code{GetToken["<eos>"]} and a parent rule \code{Assign => (expr*\{targets\}), (expr\{value\})}. Corresponding vectors for the node, rule and token stored as column vectors in matrices $W_n$, $W_r$, $W_v$.

Additionally, a decoder input contains attention and attention over history context vectors. The context vectors are calculated as described in \cref{attention}. Attention over history uses vectors from a previous decoder output:

\begin{equation}
    \phi_h^{(t)} = H_d\cdot\alpha_h^{(t)}
\end{equation}

Given all described above input values and a previous decoder embedding $h_d^{(t-1)}$, the next decoding step is calculated as following:

\begin{equation}
    h_d^{(t)}=f_{lstm}([a^{(t-1)}, n^{(t)}, p^{(t)}, \phi^{(t-1)}, \phi_h^{(t-1)}], h_d^{(t-1)})
\end{equation}

\subsection{Calculating probabilities}
In this section we explained how action probabilities from  \cref{eqn:tree_probability} are calculated from a decoder embeddings $h^{(t)}$.

\textbf{ApplyRule}. The probability of applying a rule $r$ as the current action $a^{(t)}$ is given by a softmax:

\begin{equation}
\begin{gathered}
    h^{(t)}_r = tanh(W _{r1}\cdot h^{(t)} + b_{r1}) \\
    P_r = softmax(W_r\cdot h^{(t)}_r  + b_r) \\
    p(a^{(t)} = ApplyRule[r]|x,a^{(<t)}) = P_r\cdot e(r)
\end{gathered}
\label{eqn:apply_rule}
\end{equation}

where $e(r)$ is one-hot embedding for rule $r$.

\textbf{GenToken}. As described in \cref{gentoken}, a token $v$ can be generated from a predefined vocabulary or copied from the input. Therefore probability of an action to be $GenToken[v]$ is defined as a marginal probability:
    
\begin{equation}
\begin{gathered}
    p(a^{(t)} = GenToken[v]|x,a^{(<t)}) = p(gen|x, a^{(<t)}) p(v|gen, x, a^{(<t)}) + \\
    p(copy|x, a^{(<t)}) p(v|copy, x, a^{(<t)})
\end{gathered}
\end{equation}

\begin{equation}
    p(gen|\cdot), p(copy|\cdot) = softmax(W_s\cdot h^{(t)} + b_s)
\end{equation}

Probability of selection of a token $v$ from the vocabulary is calculated similarly to \cref{eqn:apply_rule}. The difference is that the decoder embedding is concatenated with a context vector $\phi^{(t)}$:

\begin{equation}
\begin{gathered}
    h^{(t)}_v = tanh(W _{v1}\cdot [h^{(t)}, \phi^{(t)}] + b_{v1}) \\
    P_v = softmax(W_v\cdot h^{(t)}_v  + b_v) \\
    p(v|gen, x, a^{(<t)}) = P_v\cdot e(v)
\end{gathered}
\label{eq:vocab_gen}
\end{equation}

To model the copy probability we used a pointer network \parencite{Vinyals2015} described in the \cref{pointer}. The same as in \cref{eq:vocab_gen}, we used decoder embedding concatenated with the context vector:

\begin{equation}
\begin{gathered}
    P_c = f_{pointer}(H_e, [h^{(t)}, \phi^{(t)}]) \\
    p(v|copy, x, a^{(<t)}) = P_c \cdot e(v)
\end{gathered}
\end{equation}

Usage of concatenated vector $[h^{(t)}, \phi^{(t)}]$ for token generation probability calculation is reasonable since tokens are likely to occur in the input sequence, thus context vector might store important information.

\section{Training and Inference}

Values for all weights and biases were inferred from training examples, as described in \cref{eqn:mle}. Each AST $\tau$ from a training set was transformed into a sequence of ground truth actions. The next decoder input is selected from ground truth sequence (the teacher forcing, described in  \cref{seq2seq}). The model is optimized by maximizing the log-likelihood of the ground truth actions sequence. Optimization was performed by the method of error back-propagation using ADAM \parencite{Kingma2014} algorithm. At inference time, given an NL description, we used beam search, described in \cref{beam} to approximate the best AST $\hat{y}$ in \cref{eqn:main_problem}.



