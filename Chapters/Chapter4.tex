\chapter{Model} 
\label{Chapter4}

\section{Code generation problem}
Given a natural language description $x$ our task is to infer the Python code $y$ based on the intent of the $x$. This task is solved by a generation of an underlying abstract syntax tree $\tau$. $\tau$ can be deterministically converted to a Python code $y$, though in this work source code $y$ and its abstract syntax tree are considered equivalent. A probabilistic grammar model of generating an abstract syntax tree $\tau$ given description $x$ is defined as $P(\tau|x)$. The best corresponding syntax tree $\tau$ is defined as
\begin{equation}
\hat{\tau}=\underset{\tau}{\operatorname{argmax}} p(\tau|x)
\label{eqn:main_problem}
\end{equation}
Probability from Eq. \ref{eqn:main_problem} is modeled with neural model with set of weights $\theta$. To learn values of $\theta$ we used set of training examples, which consist of tuples $(\tau^{(x)}, x)$. The parameters of the model are learned by maximizing the conditional log-probabilities for the training set:
\begin{equation}
\theta=\underset{\theta}{\operatorname{argmax}} \sum_{\tau^{(x)}, x} log \: P(\tau^{(x)}|x; \theta)
\label{eqn:mle}
\end{equation}

\section{Abstract syntax tree generation}
The output of decoder is abstract syntax tree which consist of terminal and non-terminal nodes. As suggested in work of \cite{Yin2017}, we factorized the generation process of AST into sequential application of actions of two types:
\begin{itemize}
	\item \code{ApplyRule[r]} corresponds to non-terminal nodes. It applies a production rule \code{r} to the current derivation tree.
	\item \code{GetToken[v]} correspond to terminal nodes. It finishes the node by appending a token \code{v}.
\end{itemize}

Let's consider as example AST from Fig. \ref{fig:ast}. It generation consist of following actions:

\begin{codelist}{AST production sequence.}
\begin{verbnobox}[\verbarg]
ApplyRule[root => (stmt*)]
    ApplyRule[stmt => (Assign)]
        ApplyRule[Assign => (expr*{targets}), (expr{value})]
            ApplyRule[expr* => (expr)]
                ApplyRule[expr => (Name)]
                    ApplyRule[Name -> (str{id})]
                        GetToken["ls"]
                        GetToken["<eos>"]
            ApplyRule[expr => (Call)]
                ApplyRule[Call => (expr{func}), (expr*{args}), (keyword*{keywords})]
                    ApplyRule[expr => (Name)]
                        ApplyRule[Name -> (str{id})]
                            GetToken["sorted"]
                            GetToken["<eos>"]
                    ApplyRule[expr* => expr]
                        ApplyRule[expr => (Name)]
                            ApplyRule[Name => (str{id})]
                                GetToken["a"]
                                GetToken["<eos>"]
                    ApplyRule[keyword* => keyword]
                        ApplyRule[keyword => (str{arg}), (expr{value})]
                            GetToken["key"]
                            GetToken["<eos>"]
                            ApplyRule[expr => (Lambda)]
...
ApplyRule[Lambda => (arguments args), (expr body)]
    ApplyRule[arguments => (arg* args)]
        ApplyRule[arg* => arg]
            ApplyRule[arg => (str{arg})]
                GenToken["x"]
                GetToken["<eos>"]
    ApplyRule[expr => Subscript]
        ApplyRule[Subscript => (expr{value}), (slice{slice})]
            ApplyRule[expr => (Name)]
                ApplyRule[Name => (str{id})]
                    GetToken["x"]
                    GetToken["<eos>"]
            ApplyRule[slice => (Index)]
                ApplyRule[Index => (expr{value})]
                    ApplyRule[expr => (Num)]
                        ApplyRule[Num => (int{n})]
                            GetToken["1"]
                            GetToken["<eos>"]
\end{verbnobox}
\label{code:ast_production}
\end{codelist}

Under this grammar model, the probability of generating an AST $\tau$ is factorized as:
\begin{equation}
p(\tau|x) = \prod{N}{n=1} p(a_n|x, a_{<n})
\label{eqn:tree_probability}
\end{equation}
where $a_t$ is the action taken at the time step $t$ and $a_{<t}$ is the sequence of actions before $t$. 

For each time step $t$ model selects next action with maximal probability - \code{ApplyRule} to grow the tree or \code{GenToken} to fill values in terminal nodes. 

\subsection{ApplyRule actions}

At any moment of generation tree contains single frontier node (for time step $t$ it is $n_{f_t}$). Action \code{ApplyRule} expand frontier node in depth-first, left-to-right traversal of the tree. Production rule $r$ expands $n_{f_t}$ by appending all child nodes specified by the selected production. For example, in Ls.\ref{code:ast_production} in step 10 rule for node \code{Call} extends this node with three new nodes: \code{expr\{func\}}, \code{expr*\{args\}}, \code{keyword*\{keywords\})}. 
When $n_{f_t}$ is a terminal node, which can not be expanded further, the next action must be \code{GenToken}

\textbf{Unary closures}. Sometimes, generating an AST requires applying a chain of unary productions. For example, in Ls. \ref{code:ast_production} in steps 4-6 it takes three time step to generate target for \code{Assign} statement:

\begin{verbatim}
ApplyRule[expr* => (expr)]
    ApplyRule[expr => (Name)]
        ApplyRule[Name -> (str{id})]
\end{verbatim}
Such formal redundancy allows to have smaller production rule grammar but would increase model complexity. Thus, they can be replaced with one action by taking the closure of the chain of unary productions:
\begin{verbatim}
ApplyRule[expr* => (str{id})]
\end{verbatim}

We tested our model both with and without unary closures.

\subsection{GenToken actions}
If tree reached leaf and $n_{f_t}$ is terminal node, the \code{GenToken} actions used to fill this node. Each token generation ends with special end-of-string token \code{"<eos>"}. This way complex tokens like function name \code{sortBySecondIndex} can be split on parts, thus reduce token vocabulary and allow complex rare token to be constructed from its constituents. After the end-of-string token generation model proceeds to the next frontier node.

Vocabulary of predefined token values can be inferred from the dataset. However, it is clear that this vocabulary will not cover all possible token for any environment. To cope with this problem, values can be copied directly from the input sequence. Therefore, it allows model to use literals and names from the code description.

\section{Action probabilities}
Probabilities in Eq. \ref{eqn:tree_probability} estimated by neural attentional encoder-decoder model. Both encoder and decoder informational flow is structured by syntactic trees. 

\subsection{Encoder}
Input NL description $x$ consist of two parts. First is a sequence of word vectors $\{w^{(t)}\}^n_{t=1}$. Second is a description tree which consists of $m$ nodes $\{\eta^{(t)}\}^m_{t=1}$, where $m\geq n$. Details about the description tree parsing can be found in section \ref{preprocessing}. For all $t \leq n$, each tree node $\eta^{(t)}$ have a corresponding input vector from word sequences $w^{(t)}$. For $t > n$, input vector for $\eta^{(t)}$ is padding-vector. Each tree node $\eta^{(t)}$ have set of children nodes $ch(\eta^{(t)})=\{\eta_{i}\}^k_{i=1}$. Therefore a single input element $x^{(t)}$ can be defined as a tuple $(\eta^{(t)}, w^{(t)}, ch(\eta^{(t)}))$:

\begin{equation}
 x^{(t)} =
  \begin{cases}
    (\eta^{(t)}, w^{(t)}, ch(\eta^{(t)}))  & \quad \text{if } t\leq n\\
    (\eta^{(t)}, w_{pad}, ch(\eta^{(t)}))  & \quad \text{if } t > n
  \end{cases}
\label{eq:enc_input}
\end{equation}

To encode this structures we used Tree-LSTM from \cite{Tai2015}\footnote{Used pytorch implementation from \href{https://github.com/dasguptar/treelstm.pytorch}{https://github.com/dasguptar/treelstm.pytorch}}. Similary to SRvN, described in section \ref{sec:rvnn}, this model starts from tree leaves, and recursively computes node embedding $h^{(t)}$ for each $x^{(t)}$ using values of memory cell $\{c_i\}^{k}_{i=1} = memory(ch(\eta^{(t)}))$ and previous  embeddings $\{h_i\}^{k}_{i=1} = hidden(ch(\eta^{(t)}))$ from children nodes:

\begin{equation}
\begin{gathered}
    \hat{h} = \sum^{k}_{i=1}h_i \\
    
    i^{(t)} = \sigma(W_i\cdot[\hat{h}, w^{(t)}]+b_i) \\
    
    u^{(t)} = tanh(W_u\cdot[\hat{h}, w^{(t)}]+b_u) \\
    
    f^{(t)}_i = \sigma(W_{f}\cdot [h_i, w^{(t)}] + b_f) \\
    
    c^{(t)} = i^{(t)} \circ u^{(t)} + \sum_{i=1}^{k} f^{(t)}_i \circ c_i \\
    
    o^{(t)} = \sigma(W_o\cdot[\hat{h}, w^{(t)}]+b_o) \\
    
    h^{(t)} = o^{(t)} \circ tanh(c^{(t)})

\end{gathered}
\label{eq:tree_lstm}
\end{equation}

\subsection{Decoder}
The decoder is RNN which sequentially generates AST model as defined in Eq. \ref{eqn:tree_probability}. Each production action naturally grounds to a step in the decoder. This way, the sequence of production rules from Ls. \ref{code:ast_production} can be interpreted as unrolling RNN time steps with some additional connections from parent action steps.

We used implementation of decoder from \cite{Yin2017}. It is vanilla LSTM with additional connections which reflect the topological structure of the code syntax. For each decoding step input vector is concatenation of frontier node embedding $n^{(t)}$, previous action embedding $a^{(t-1)}$ and parent feeding $p^{(t)}$. Parent feeding is a concatenation of decoder hidden state from parent step $h_{dp}^({t})$ and parent rule embedding $r_p^({t})$. Consider as example step 9 in Ls.\ref{code:ast_production}: 
    
\begin{verbatim}
ApplyRule[Assign => (expr*{targets}), (expr{value})]
    ...............
                GetToken["<eos>"]
    ApplyRule[expr => (Call)]
\end{verbatim}

It has frontier node \code{expr}, previous action \code{GetToken["<eos>"]} and parent rule \code{Assign => (expr*\{targets\}), (expr\{value\})}. Corresponding vectors for node, rule and token stored as column vector in matrices $W_n$, $W_r$, $W_t$.

Additionally decoder input contains attention and attention over history context vectors. Context vectors calculated as described in section \ref{attention}. Attention over history creates context vector from previous decoder output:

\begin{equation}
    \phi_h^{(t-1)} = H_d\cdot\alpha_h^{(t-1)}
\end{equation}

Given all described above input values and previous decoder embedding $h_d^{(t-1)}$, next decoding step is calculated as following:

\begin{equation}
    h_d^{(t)}=lstm([a^{(t-1)}, n^{(t)}, p^{(t)}, \phi^{(t-1)}, \phi_h^{(t-1)}], h_d^{(t-1)})
\end{equation}



\section{Learning}
Beam search.