\chapter{Experiments} \label{Chapter5} 

\section{Datasets}
\textbf{HearthStone (HS)}  dataset \parencite{Ling2016} is a collection of Python classes which implements cards from the card game HearthStone. Each card has a set of attributes which is concatenated to produce the input sequence. The dataset contains 665 Python classes with descriptions.

\textbf{Django} dataset \parencite{Oda2015} contains a corpus of lines of Python code with manually annotated pseudo-code from the Django web framework. Corpus contains 18,805 pairs of Python statements and corresponding English pseudo-codes. 

Statistics of datasets can be found in Tab. \ref{tab:stat}.

\begin{table}
\centering
\begin{tabular}{ l l l }
\hline
\textbf{Dataset} & \textbf{HS} & \textbf{Django} \\
\hline 
Train & 533 & 16.000 \\ 
Development & 66 & 1.000 \\ 
Test & 66 & 1.805 \\ 
\hline
Avg. tokens in description$^*$ & 47.3 & 13.7 \\
Avg. nodes in constituency tree & 93.5 & 26.3 \\
Avg. nodes in CCG tree & 109.4 & 28 \\
Avg. characters in code & 324.3 & 43.9 \\
Avg. size of AST (\# nodes) & 66.4 & 9.5 \\
 \hline
 \hline
\multicolumn{3}{c}{Statistics of Grammar} \\
terminal vocabulary size & 550 & 3466 \\ 
\hline
\multicolumn{3}{l}{\textbf{w/o unary closures}} \\
\# productions$^\dagger$ & 100 & 222 \\
\# node types$^\dagger$ & 61 & 96 \\
Avg. \# of actions per example$^\dagger$ & 173.4 & 20.3 \\ 
\hline
\multicolumn{3}{l}{\textbf{w/ unary closures}} \\
\# productions$^\dagger$ & 100 & 237 \\
\# node types$^\dagger$ & 57 & 92 \\
Avg. \# of actions per example$^\dagger$ & 141.7 & 16.4 \\ 
\hline
\end{tabular}
\caption[Statistics of datasets]{Statistics of datasets and associated grammars ($^\dagger$Previously reported by \cite{Yin2017}. $^*$Number of dependency tree nodes is equal to a number of tokens in the description.)}
\label{tab:stat}
\end{table}

% \cite{Barone2017} (BS) created the dataset of Python code with parallel descriptions from GitHub. It contains 150,370 triples of function declarations, docstrings and bodies. This dataset presents the most challenging task because it contains highly heterogeneous and noisy data. Django pseudo-code already has a good alignment with target code and HeartStone code examples are mostly homogeneous and target specific domain.

\subsection{Preprocessing} \label{preprocessing}

All input descriptions was tokenized using Stanford CoreNLP\footnote{\href{https://stanfordnlp.github.io/CoreNLP}{https://stanfordnlp.github.io/CoreNLP}} Java package. Quoted text, which might be referred as values for string constants, replaced with special markers (Tab. \ref{table:str_markers}). Nested object references in queries, like \code{re.findall} was split by the period so pointer network can copy each part separately (Tab. \ref{table:function_calls}). For \textbf{HS} we also constructed synthetic description, using structured part of target class descriptions (Tab. \ref{table:hs_input}). Then all descriptions were parsed to trees with three different approaches, described below.

\begin{table}
\begin{tabularx}{\textwidth}{ l X }
\hline
\textbf{Input query:} & while '<' is contained in value and '>' is contained in value, \\
\hline 
\textbf{Input query preprocessed:} & while \_STR\_0\_ is contained in value and \_STR\_1\_ is contained in value , \\
\hline 
\textbf{Target code:} & while '<' in value and '>' in value: \\
\hline 
\textbf{Target code preprocessed:} & while '\_STR\_0\_' in value and '\_STR\_1\_' in value: \\
\hline
\end{tabularx}
\caption[Quoted items preprocessing]{Quoted items preprocessing for item \#2 from the development split of \textbf{Django}.}
\label{table:str_markers}
\end{table}

\begin{table}
\begin{tabularx}{\textwidth}{ l X }
\hline
\textbf{Input query:} & from django.utils.six.moves import html\_parser as \_html\_parse into default name space. \\
\hline 
\textbf{Input query preprocessed:} & from django.utils.six.moves ( django utils six moves ) import html\_parser as \_html\_parse into default name space . \\
\hline 
\end{tabularx}
\caption[Nested object references preprocessing]{Nested bject references preprocessing for item \#93 from the developer split of \textbf{Django}.}
\label{table:function_calls}
\end{table}

\begin{table}
\begin{tabularx}{\textwidth}{ l X }
\hline
\textbf{Structured input:} & Deadly Poison NAME\_END -1 ATK\_END -1 DEF\_END 1 COST\_END -1 DUR\_END Spell TYPE\_END Rogue PLAYER\_CLS\_END NIL RACE\_END Free RARITY\_END Give your weapon +2 Attack. \\
\hline 
\textbf{Synthetic description:} & Name: Deadly Poison, attack: -1, defence: -1, cost: 1, duration: -1, type: Spell, player class: Rogue, race: None, rarity: Free. Give your weapon +2 Attack. \\
\hline
\end{tabularx}
\caption[Synthetic description example]{Synthetic description for the item \#3 from the developer split of \textbf{HS}.}
\label{table:hs_input}
\end{table}

To create CFG sentence representation we used \code{LexicalizedParser} \parencite{klein2003} from the CoreNLP. Dependency parsing was done by \code{DependencyParser} \parencite{chen2014} from the CoreNLP. For CCG parsing we used package EasyCCG\footnote{\href{http://homepages.inf.ed.ac.uk/s1049478/easyccg.html}{http://homepages.inf.ed.ac.uk/s1049478/easyccg.html}} \parencite{lewis2014}.\footnote{We were not able to include figures with parsed trees examples in this thesis due to their large size, but you can find them at our \href{https://github.com/tsdaemon/treelstm-code-generation/tree/master/pictures}{GitHub repo}}

\section{Implementation details}

\textbf{Dynamic computational graph.} Model of \cite{Yin2017} was build on framework Theano\footnote{\href{http://deeplearning.net/software/theano/}{http://deeplearning.net/software/theano/}}. But Theano is not able to build a dynamic computational graph to encode syntactic trees. Therefore, we have implemented our model on PyTorch\footnote{\href{http://pytorch.org/}{http://pytorch.org/}}.


\textbf{Mini-batch training.} The nature of the recursive tree encoding does not allow to process data in batches, as each query defines unique computational graph. But other parts of the model was able to perform batch operations on data. Therefore we used a wrapper encoder module, which splitted input batch on single queries, processed them with Tree-LSTM module sequentially and combined back into batch. We used batches of size 10 for \textbf{HS} and 50 for \textbf{Django}.

\textbf{Model parameters.} The sizes of nodes, rules and terminal embeddings were 256. Except for the word embeddings, for which it was 300. We used pre-trained Common Crawl GloVe vectors \parencite{pennington2014} for the word embeddings weights. We were not freezing this weights, so the pre-trained values could be additionally adjusted during training. The dimensions of the encoder and decoder hidden states and memory cells were 256. Hidden states of attention and pointer networks were of size 50. Also, we used the last state of the encoder as the initial state of the decoder (thought vector). For decoding, we used the beam of size 10.

% https://r2rt.com/non-zero-initial-states-for-recurrent-neural-networks.html
\textbf{Regularization.} Since our datasets were relatively small for such complex neural model, we added strong regularization using Variational Dropout suggested in work of \cite{Gal2016}. Similarly to approach described in the work of \cite{zimmermann2012} we were adding Gaussian noise with mean 0.0 and STD 0.1 to initial states $h^{(0)}$ and $c^{(0)}$ of encoder. These methods introduced significant improvement in both training speed and evaluation scores.

\section{Experimental setup} \label{exp_setup}

\textbf{Evaluation metrics.} For this experiment, we measured \textbf{accuracy} as a fraction of output code which fully matches target examples. Additionally, to measure the quality of examples without full match we used average token level \textbf{BLEU-4}, as suggested by \cite{Ling2016} and \cite{Yin2017}. However, BLEU and accuracy do not measure the actual correctness of a generated code. Therefore we defined  \textbf{errors} metric as a fraction of output trees which we were not able to convert into code.

\textbf{Baseline.} Along with Tree-LSTM encoder, we build bidirectional LSTM encoder, previously described in the work of \cite{Yin2017}. This was done to have a clear baseline for our tree encoding method. 

\section{Results}
Evaluation results are listed in Tab. \ref{table:evaluation}. We compared our results with two approaches: (1) Latent Predictor Network (LPN) of \cite{Ling2016} and (2) Syntactic Neural Model of \cite{Yin2017}. 

\textbf{Analysis.} As presented in Tab. \ref{table:evaluation}, the performance of models with Tree-LSTM encoder have results comparable with LPN. Yet no model was able to improve current state-of-the-art results of Syntactic Neural Model. Also, tree encoders have shown a much higher rate of errors than BiLSTM encoder.

Results of the model with BiLSTM encoder match the previously reported results of \cite{Yin2017}. This makes us certain that our implementation is valid and difference in performance with previous results caused actually be the tree encoders.

Dependency trees encoding has shown better results for \textbf{HS}. Dependency trees do not contain phrasal nodes and therefore shorter. This can be a proper justification for the difference in results for \textbf{HS} which contains long and similar descriptions. However, results for the \textbf{Django} dataset  have a contrary bias. The performance of dependency trees is much lower than the performance of CFG and CCG trees. It can be explained by the fact, that \textbf{Django} has shorter and more divergent queries and due to this properties its dependency trees has lower generalization ability. CCG and CFG trees have less divergent structures where each node have no more than two children. Therefore informational flow in such trees might have higher approximation abilities. Yet this is only our assumption and it requires additional research.

% \section{Case studies}

% \begin{table}[h]
% \begin{tabular}{ l m{5cm} }
% \hline
% \textbf{Query:} &  \\
% \hline 
% \hline 
% \textbf{Sequential encoder results:} & 
% \begin{verbatim} 
% tralivali 
% ululu
% \end{verbatim}
% \\
% \hline
% \textbf{Tree encoding result:} & \\
% \hline
% \ \textbf{dependency tree} & 
% % \begin{verbatim} 
% % tralivali 
% % ululu
% % \end{verbatim} 
% \\
% \hline
% \ \textbf{constituency tree} & 
% % \begin{verbatim} 
% % tralivali 
% % ululu
% % \end{verbatim} 
% \\
% \hline
% \ \textbf{CCG tree} & 
% % \begin{verbatim} 
% % tralivali 
% % ululu
% % \end{verbatim} 
% \\
% \hline
% \end{tabular}
% \caption[Translation example for HS]{Translation example for the item \#4 from the test split of \textbf{HS}.}
% \label{table:hs_case}
% \end{table}


\begin{table}[p]
\begin{tabular}{ l c c c c c c }

\hline
& \multicolumn{3}{c}{\textbf{HS}} & \multicolumn{3}{c}{\textbf{Django}}\\
\hline
& ACC & BLEU & ERROR & ACC & BLEU & ERROR \\
\hline
Retrieval system:$^\dagger$ & 0.0 & 62.5 & - & 14.7 & 18.6 & - \\
Phrasal statistical MT:$^\dagger$ & 0.0 & 34.1 & - & 31.5 & 47.6 & - \\
Hierarchical statistical MT:$^\dagger$ & 0.0 & 43.2 & - & 9.5 & 35.9 & - \\
\hline 
NMT$^\ddagger$ & 1.5 & 60.4 & - & 45.1 & 63.4 & - \\
Seq2Tree$^\ddagger$ & 1.5 & 53.4 & - & 28.9 & 44.6 & - \\
Seq2Tree-UNK$^\ddagger$ & 13.6 & 62.8 & - & 39.4 & 58.2 & - \\
LPN$^\dagger$ & 4.5 & 65.6 & - & 62.3 & 77.6 & - \\
Syntactic Neural Model$^\ddagger$ & \textbf{16.7} & \textbf{75.8} & - & \textbf{71.6} & \textbf{84.5} & - \\
\hline
\multicolumn{7}{l}{\textbf{w/ unary closures:}} \\
Bi-directional LSTM encoder & 9.1 & 71.6 & 0.6 & 68 & 82.5 & 0.8  \\
\multicolumn{7}{l}{Tree-LSTM encoder} \\
\ with dependency trees & 4.5 & 66.7 & 3.0 & 32.9 & 55.5 & 1.1 \\
\ with constituency trees & 4.5 & 63.9 & 13.2 & 42.0 & 61.8 & 0.9 \\
\ with CCG trees & 3.0 & 66.1 & 5.1 & 48.7 & 68.7 & 1.5 \\
\multicolumn{7}{l}{\textbf{w/o unary closures:}}\\
Bi-directional LSTM encoder & 16.2 & 74.2 & 0.3 & 71.0 & 84.5 & 0.4  \\
\multicolumn{7}{l}{Tree-LSTM encoder} \\
\ with dependency trees & 6 & 71.5 & 4.0 & 32.0 & 55.0 & 0.9 \\
\ with constituency trees & 4.5 & 64.9 & 13.2 & 42.3 & 61.0 & 1.1 \\
\ with CCG trees & 3.0 & 65.3 & 11.0 & 49.4 & 69.6 & 1.8 \\
\hline
\end{tabular}
\caption[Final results]{Evaluation results for both datasets. $^\dagger$Results previously reported by \cite{Ling2016}. $^\ddagger$Results previously reported by \cite{Yin2017}.}
\label{table:evaluation}
\end{table}




