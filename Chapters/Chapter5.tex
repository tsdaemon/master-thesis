\chapter{Experiments} \label{Chapter5} 


\section{Datasets preprocessing} \label{preprocessing}
% A* CCG Parsing with a Supertag and Dependency Factored Model
% https://nlp.stanford.edu/pubs/glove.pdf 

\section{Evaluation metrics}
BLEU
Accuracy
Error

rewrite: These two metrics are not ideal: accuracy only measures exact match and thus lacks the ability to give credit to semantically correct code that is different from the reference, while it is not clear whether BLEU provides an appropriate proxy for measuring semantics in the code generation task. A more
intriguing metric would be directly measuring semantic/functional code equivalence, for which we present a pilot study
at the end of this section (cf. Error Analysis). We leave exploring more sophisticated metrics (e.g. based on static code
analysis) as future work.

\section{Results}

Describe previous results. Could be used results from \cite{Yin2017} and \cite{Barone2017}.


