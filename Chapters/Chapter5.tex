\chapter{Experiments} \label{Chapter5} 

\section{Datasets}
\textbf{HearthStone (HS)}  dataset \parencite{Ling2016} is a collection of Python classes which implements cards from the card game HearthStone. Each card has a set of attributes which is concatenated to produce an input sequence. The dataset contains 665 Python classes with descriptions.

\textbf{Django} dataset \parencite{Oda2015} contains a corpus of lines of Python code with manually annotated pseudo-code from the Django web framework. Corpus contains 18,805 pairs of Python statements and corresponding English pseudo-codes. 

Additional information about the datasets can be found in \cref{tab:stat}.

\begin{table}
\centering
\begin{tabular}{ l l l }
\hline
\textbf{Dataset} & \textbf{HS} & \textbf{Django} \\
\hline 
Train & 533 & 16.000 \\ 
Development & 66 & 1.000 \\ 
Test & 66 & 1.805 \\ 
\hline
Avg. tokens in description$^\ddagger$ & 47.3 & 13.7 \\
Avg. nodes in constituency tree & 93.5 & 26.3 \\
Avg. nodes in CCG tree & 109.4 & 28 \\
Avg. characters in code & 324.3 & 43.9 \\
Avg. size of AST (\# nodes) & 66.4 & 9.5 \\
 \hline
 \hline
\multicolumn{3}{c}{Statistics of Grammar} \\
terminal vocabulary size & 550 & 3466 \\ 
\hline
\multicolumn{3}{l}{\textbf{w/o unary closures}} \\
\# productions$^\dagger$ & 100 & 222 \\
\# node types$^\dagger$ & 61 & 96 \\
Avg. \# of actions per example$^\dagger$ & 173.4 & 20.3 \\ 
\hline
\multicolumn{3}{l}{\textbf{w/ unary closures}} \\
\# productions$^\dagger$ & 100 & 237 \\
\# node types$^\dagger$ & 57 & 92 \\
Avg. \# of actions per example$^\dagger$ & 141.7 & 16.4 \\ 
\hline
\end{tabular}
\caption[Statistics of datasets]{Statistics of datasets and associated grammars ($^\dagger$Previously reported by \cite{Yin2017}. $^\ddagger$Number of dependency tree nodes is equal to a number of tokens in the description.)}
\label{tab:stat}
\end{table}

% \cite{Barone2017} (BS) created the dataset of Python code with parallel descriptions from GitHub. It contains 150,370 triples of function declarations, docstrings and bodies. This dataset presents the most challenging task because it contains highly heterogeneous and noisy data. Django pseudo-code already has a good alignment with target code and HeartStone code examples are mostly homogeneous and target specific domain.

\subsection{Preprocessing} \label{preprocessing}

All input descriptions was tokenized using Stanford CoreNLP\footnote{\href{https://stanfordnlp.github.io/CoreNLP}{https://stanfordnlp.github.io/CoreNLP}} Java package. Quoted text, which might be referred as values for string constants, was replaced with special markers (see \cref{table:str_markers}). Nested object references in queries, like \code{re.findall} was split by the period so the pointer network can copy each part separately (see \cref{table:function_calls}). For \textbf{HS} we also constructed synthetic descriptions, using structured parts of target class descriptions (see \cref{table:hs_input}). Then all descriptions were parsed to trees with three different approaches, described below.

\begin{table}
\begin{tabularx}{\textwidth}{ l X }
\hline
\textbf{Input query:} & while '<' is contained in value and '>' is contained in value, \\
\hline 
\textbf{Input query preprocessed:} & while \_STR\_0\_ is contained in value and \_STR\_1\_ is contained in value , \\
\hline 
\textbf{Target code:} & while '<' in value and '>' in value: \\
\hline 
\textbf{Target code preprocessed:} & while '\_STR\_0\_' in value and '\_STR\_1\_' in value: \\
\hline
\end{tabularx}
\caption[Quoted items preprocessing]{Quoted items preprocessing for item \#2 from the development split of \textbf{Django}.}
\label{table:str_markers}
\end{table}

\begin{table}
\begin{tabularx}{\textwidth}{ l X }
\hline
\textbf{Input query:} & from django.utils.six.moves import html\_parser as \_html\_parse into default name space. \\
\hline 
\textbf{Input query preprocessed:} & from django.utils.six.moves ( django utils six moves ) import html\_parser as \_html\_parse into default name space . \\
\hline 
\end{tabularx}
\caption[Nested object references preprocessing]{Nested bject references preprocessing for item \#93 from the developer split of \textbf{Django}.}
\label{table:function_calls}
\end{table}

\begin{table}
\begin{tabularx}{\textwidth}{ l X }
\hline
\textbf{Structured input:} & Deadly Poison NAME\_END -1 ATK\_END -1 DEF\_END 1 COST\_END -1 DUR\_END Spell TYPE\_END Rogue PLAYER\_CLS\_END NIL RACE\_END Free RARITY\_END Give your weapon +2 Attack. \\
\hline 
\textbf{Synthetic description:} & Name: Deadly Poison, attack: -1, defence: -1, cost: 1, duration: -1, type: Spell, player class: Rogue, race: None, rarity: Free. Give your weapon +2 Attack. \\
\hline
\end{tabularx}
\caption[Synthetic description example]{Synthetic description for the item \#3 from the developer split of \textbf{HS}.}
\label{table:hs_input}
\end{table}

To create CFG sentence representation we used \code{LexicalizedParser} \parencite{klein2003} from the CoreNLP. Dependency parsing was done by \code{DependencyParser} \parencite{chen2014} from the CoreNLP. For CCG parsing we used package EasyCCG\footnote{\href{http://homepages.inf.ed.ac.uk/s1049478/easyccg.html}{http://homepages.inf.ed.ac.uk/s1049478/easyccg.html}} \parencite{lewis2014}.\footnote{We were not able to include figures with parsed trees examples in this thesis due to their large size, but you can find them at our \href{https://github.com/tsdaemon/treelstm-code-generation/tree/master/pictures}{GitHub repo}}

\section{Implementation details}

\textbf{Dynamic computational graph.} Model of \cite{Yin2017} was build on the framework Theano\footnote{\href{http://deeplearning.net/software/theano/}{http://deeplearning.net/software/theano/}}. But Theano is not able to build a dynamic computational graph to encode syntactic trees. Therefore, we have implemented our model on PyTorch\footnote{\href{http://pytorch.org/}{http://pytorch.org/}}.


\textbf{Mini-batch training.} The nature of a recursive tree encoding does not allows to process data in batches, since each query defines a unique computational graph. But other components of the model was able to perform batch operations on data. Therefore we used a wrapper module for encoder, which was splitting input batches on single queries, processed them with Tree-LSTM module sequentially and combined back into batch. We used batches of size 10 for \textbf{HS} and 50 for \textbf{Django}.

\textbf{Model parameters.} The sizes of nodes, rules and terminal embeddings were 256. Except for the word embeddings, for which it was 300. We used pre-trained Common Crawl GloVe vectors \parencite{pennington2014} for the word embeddings weights. We were not freezing this weights, so the pre-trained values could be additionally adjusted during the training. The dimensions of the encoder and decoder hidden states and memory cells were 256. Hidden states of the attention and the pointer networks were of size 50. Also, we used the last state of the encoder as the initial state of the decoder (thought vector). For decoding, we used the beam of size 10.

% https://r2rt.com/non-zero-initial-states-for-recurrent-neural-networks.html
\textbf{Regularization.} Since our datasets were relatively small for such complex neural model, we added strong regularization using Variational Dropout suggested in work of \cite{Gal2016}. Similarly to the approach described in the work of \cite{zimmermann2012} we were adding Gaussian noise with mean 0.0 and STD 0.1 to the initial states $h^{(0)}$ and $c^{(0)}$ of encoder. These methods introduced significant improvement in both training speed and evaluation scores.

\section{Experimental setup} \label{exp_setup}

\textbf{Evaluation metrics.} For this experiment, we measured \textbf{accuracy} as a fraction of output code which fully matches target examples. Additionally, to measure the quality of examples without full match we used average token level \textbf{BLEU-4}, as suggested by \cite{Ling2016} and \cite{Yin2017}. However, BLEU and accuracy do not measure the actual correctness of a generated code. Therefore we defined  \textbf{errors} metric as a fraction of output trees which we were not able to convert into a code.

\textbf{Baseline.} Along with the Tree-LSTM encoder, we build a bidirectional LSTM encoder, previously described in the work of \cite{Yin2017}. This was done to have a clear baseline for our tree encoding method. 

\begin{table}[p]
\begin{tabular}{ l c c c c c c }

\hline
& \multicolumn{3}{c}{\textbf{HS}} & \multicolumn{3}{c}{\textbf{Django}}\\
\hline
& ACC & BLEU & ERROR & ACC & BLEU & ERROR \\
\hline
Retrieval system:$^\dagger$ & 0.0 & 62.5 & - & 14.7 & 18.6 & - \\
Phrasal statistical MT:$^\dagger$ & 0.0 & 34.1 & - & 31.5 & 47.6 & - \\
Hierarchical statistical MT:$^\dagger$ & 0.0 & 43.2 & - & 9.5 & 35.9 & - \\
\hline 
NMT$^\ddagger$ & 1.5 & 60.4 & - & 45.1 & 63.4 & - \\
Seq2Tree$^\ddagger$ & 1.5 & 53.4 & - & 28.9 & 44.6 & - \\
Seq2Tree-UNK$^\ddagger$ & 13.6 & 62.8 & - & 39.4 & 58.2 & - \\
LPN$^\dagger$ & 4.5 & 65.6 & - & 62.3 & 77.6 & - \\
Syntactic Neural Model$^\ddagger$ & \textbf{16.7} & \textbf{75.8} & - & \textbf{71.6} & \textbf{84.5} & - \\
\hline
\multicolumn{7}{l}{\textbf{w/ unary closures:}} \\
Bi-directional LSTM encoder & 9.1 & 71.6 & 0.6 & 68 & 82.5 & 0.8  \\
\multicolumn{7}{l}{Tree-LSTM encoder} \\
\ with dependency trees & 4.5 & 66.7 & 3.0 & 32.9 & 55.5 & 1.1 \\
\ with constituency trees & 4.5 & 63.9 & 13.2 & 42.0 & 61.8 & 0.9 \\
\ with CCG trees & 3.0 & 66.1 & 5.1 & 48.7 & 68.7 & 1.5 \\
\multicolumn{7}{l}{\textbf{w/o unary closures:}}\\
Bi-directional LSTM encoder & 16.2 & 74.2 & 0.3 & 71.0 & 84.5 & 0.4  \\
\multicolumn{7}{l}{Tree-LSTM encoder} \\
\ with dependency trees & 6 & 71.5 & 4.0 & 32.0 & 55.0 & 0.9 \\
\ with constituency trees & 4.5 & 64.9 & 13.2 & 42.3 & 61.0 & 1.1 \\
\ with CCG trees & 3.0 & 65.3 & 11.0 & 49.4 & 69.6 & 1.8 \\
\hline
\end{tabular}
\caption[Final results]{Evaluation results for both datasets. $^\dagger$Results previously reported by \cite{Ling2016}. $^\ddagger$Results previously reported by \cite{Yin2017}.}
\label{table:evaluation}
\end{table}

\section{Results}
We compared our results with two approaches: (1) Latent Predictor Network (LPN) of \cite{Ling2016} and (2) Syntactic Neural Model of \cite{Yin2017}. 

As presented in \cref{table:evaluation}, the performance of the models with the Tree-LSTM encoder have results comparable with LPN. Yet no model was able to improve current state-of-the-art results of Syntactic Neural Model. Also, tree encoders have shown a much higher rate of errors than the BiLSTM encoder.

Results of the model with BiLSTM encoder match the previously reported results of Syntactic Neural Model. This makes us certain that our implementation is valid and difference in performance with previous results caused actually by our tree encoders.

The dependency trees encoding has shown better results for \textbf{HS}. Dependency trees do not contain phrasal nodes and therefore they are shorter. This can be a proper justification for the difference in results for \textbf{HS} which contains long and homogeneous descriptions. However, results for the \textbf{Django} have a contrary bias. The performance of dependency trees is much lower than the performance of CFG and CCG trees. It can be explained by a fact, that \textbf{Django} has shorter and more divergent queries and due to this properties its dependency trees have lower generalization ability. CCG and CFG trees have less divergent structures where each node have no more than two children. Therefore informational flow in such trees might have higher approximation abilities. Yet this is only an assumption and requires additional research.

\section{Case studies}

To provide a clear understanding of the model performance we decided to present and comment  few code generation examples from both datasets.

As can be seen in listing \ref{code:hs1}, the model was able to translate the intention of the card to deal 3 damage on the line 17. However, it was not able to produce a long sequence of actions from the lines 28-31, required to create a list of all targets. Probably, this mistake is connected to the grammar model problem that we described in \cref{ast_gen}.

\begin{codelist}{Code generated for the item \#5 from the test set of \textbf{HS}}
\begin{verbnobox}[\verbarg]
# Query:
Name: Hellfire, attack: -1, defence: -1, cost: 4, 
duration: -1, type: Spell, player class: Warlock, 
race: None, rarity: Free. 
Deal $3 damage to ALL characters.

# Reference code:
class Hellfire(SpellCard):

    def __init__(self):
        super().__init__('Hellfire', 4, CHARACTER_CLASS.WARLOCK,
            CARD_RARITY.FREE)

    def use(self, player, game):
        super().use(player, game)
        targets = copy.copy(game.other_player.minions)
        targets.extend(game.current_player.minions)
        targets.append(game.other_player.hero)
        targets.append(game.current_player.hero)
        for minion in targets:
            minion.damage(player.effective_spell_damage(3), self)

# Predicted code:
class Hellfire(SpellCard):

    def __init__(self):
        super().__init__('Hellfire', 4, CHARACTER_CLASS.WARLOCK,
            CARD_RARITY.RARE, target_func=hearthbreaker.targeting.
            find_spell_target)

    def use(self, player, game):
        super().use(player, game)
        self.target.damage(player.effective_spell_damage(3), self)
        
\end{verbnobox}
\label{code:hs1}
\end{codelist}

In listing \ref{code:hs2}, you can see that model was mo able to understand complex script for a Murloc behavior from line 5. While the model was able to learn the alignment between simple query properties and the target class, really complex instructions still is not comprehensible. 

\begin{codelist}{Code generated for the item \#27 from the test set of \textbf{HS}}
\begin{verbnobox}[\verbarg]
# Query:
Name: Siltfin Spiritwalker, attack: 2, defence: 5, 
cost: 4, duration: -1, type: Minion, 
player class: Shaman, race : Murloc, rarity: Epic. 
Whenever another friendly Murloc dies, draw a card.
Overload: (1)

# Reference code:
class SiltfinSpiritwalker(MinionCard):

    def __init__(self):
        super().__init__('Siltfin Spiritwalker', 4, CHARACTER_CLASS.SHAMAN,
            CARD_RARITY.EPIC, minion_type=MINION_TYPE.MURLOC, overload=1)

    def create_minion(self, player):
        return Minion(2, 5, effects=[Effect(MinionDied(IsType(MINION_TYPE.
            MURLOC)), ActionTag(Draw(), PlayerSelector()))])

# Predicted code:
class SiltfinSpiritwalker(MinionCard):

    def __init__(self):
        super().__init__('Siltfin Spiritwalker', 4, CHARACTER_CLASS.SHAMAN,
            CARD_RARITY.EPIC, battlecry=Battlecry(Give(Buff(ManaChange(-1))
            ), CardSelector(condition=IsSpell())))

    def create_minion(self, player):
        return Minion(2, 2)
        
\end{verbnobox}
\label{code:hs2}
\end{codelist}

In listings \ref{code:django1} and \ref{code:django2} we presented a comparison of results from the models with different encoders. The result from the baseline model with BiLSTM encoder almost matched the target in both cases. However, a predicted code has not all elements from a reference code. This can be a result of inability of the grammar to produce the lists of arbitrary lengths, described in \cref{ast_gen}. Still, the model with BiLSTM encoder produced the code with a much higher quality than the models with the tree encoders.

\begin{codelist}{Code generated for the item \#665 from the test set of \textbf{Django}}
\begin{verbnobox}[\verbarg]
# Query:
call the method self._text_chars with 4 arguments: length,
truncate, text and truncate_len, return the result.

# Reference code:
return self._text_chars(length, truncate, text, truncate_len)

# Code, predicted with BiLSTM encoder:
return self._text_chars(length, truncate, text)
        
# Code, predicted with tree encoder and CCG trees:
return self.clear_checkbox_id(text, text)

# Code, predicted with tree encoder and dependency trees:
return self._text_chars(text, text)
\end{verbnobox}
\label{code:django1}
\end{codelist}

\begin{codelist}{Code generated for the item \#924 from the test set of \textbf{Django}}
\begin{verbnobox}[\verbarg]
# Query:
tt is a tuple with 9 elements: dt.year, dt.month,
dt.day, dt.hour, dt.minute, dt.second, 
result of the method dt.weekday,

# Reference code:
tt = dt.year, dt.month, dt.day, dt.hour, 
    dt.minute, dt.second, dt.weekday(), 0, 0

# Code, predicted with BiLSTM encoder:
tt = dt.year, dt.month, dt.day
        
# Code, predicted with tree encoder and dependency trees:
timetuple = dt.year, dt.timetuple, dt.timetuple

# Code, predicted with tree encoder and CFG trees:
date_data = dt.year, dt.year, dt.microsecond
\end{verbnobox}
\label{code:django2}
\end{codelist}





