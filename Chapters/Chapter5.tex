\chapter{Model evaluation} 
\label{Chapter5} 

Describe metrics

Describe dataset

Describe previous methods. Could be used results from \cite{Yin2017} and \cite{Barone2017}.

mean reciprocal rank

rewrite: These two metrics are not ideal: accuracy only measures exact match and thus lacks the ability to give credit to semantically correct code that is different from the reference, while it is not clear whether BLEU provides an appropriate proxy for measuring semantics in the code generation task. A more
intriguing metric would be directly measuring semantic/functional code equivalence, for which we present a pilot study
at the end of this section (cf. Error Analysis). We leave exploring more sophisticated metrics (e.g. based on static code
analysis) as future work.