\chapter{Experiments} \label{Chapter5} 

\section{Datasets}
\textbf{HearthStone (HS)}  dataset \parencite{Ling2016} is a collection of Python classes which implements cards from the card game HearthStone. Each card has a set of attributes which is concatenated to produce the input sequence. The dataset contains 665 Python classes with descriptions.

\textbf{Django} dataset \parencite{Oda2015} contains a corpus of lines of Python code with manually annotated pseudocode from the Django web framework. Corpus contains 18,805 pairs of Python statements and corresponding English pseudo-codes. 

\begin{table}
\centering
\begin{tabular}{ l l l }
\hline
\textbf{Dataset} & \textbf{HS} & \textbf{Django} \\
\hline 
Train & 533 & 16.000 \\ 
Development & 66 & 1.000 \\ 
Test & 66 & 1.805 \\ 
\hline
Avg. tokens in description$^*$ & 47.3 & 13.7 \\
Avg. nodes in constituency tree & 93.5 & 26.3 \\
Avg. nodes in CCG tree & 109.4 & 28 \\
Avg. characters in code & 324.3 & 43.9 \\
Avg. size of AST (\# nodes) & 66.4 & 9.5 \\
 \hline
 \hline
\multicolumn{3}{c}{Statistics of Grammar} \\
terminal vocabulary size & 550 & 3466 \\ 
\hline
\multicolumn{3}{l}{\textbf{w/o unary closures}} \\
\# productions$^\dagger$ & 100 & 222 \\
\# node types$^\dagger$ & 61 & 96 \\
Avg. \# of actions per example$^\dagger$ & 173.4 & 20.3 \\ 
\hline
\multicolumn{3}{l}{\textbf{w/ unary closures}} \\
\# productions$^\dagger$ & 100 & 237 \\
\# node types$^\dagger$ & 57 & 92 \\
Avg. \# of actions per example$^\dagger$ & 141.7 & 16.4 \\ 
\hline
\end{tabular}
\caption[Statistics of datasets]{Statistics of datasets and associated grammars ($^\dagger$Previously reported by \cite{Yin2017}. $^*$Number of dependency tree nodes is equal to a number of tokens in descrition.)}
\end{table}

% \cite{Barone2017} (BS) created the dataset of Python code with parallel descriptions from GitHub. It contains 150,370 triples of function declarations, docstrings and bodies. This dataset presents the most challenging task because it contains highly heterogeneous and noisy data. Django pseudo-code already has a good alignment with target code and HeartStone code examples are mostly homogeneous and target specific domain.

\subsection{Preprocessing} \label{preprocessing}

All input descriptions was tokenized using Stanford CoreNLP\footnote{\href{https://stanfordnlp.github.io/CoreNLP}{https://stanfordnlp.github.io/CoreNLP}} Java package. Quoted text, which might be refered as values for string constants, replaced with special markers (Tab. \ref{table:str_markers}). Nested object references, like \code{re.findall} was split by the period so pointer network can copy each part separately (Tab. \ref{table:function_calls}). For \textbf{HS} we also constructed synthetic description, using structured part of target class descriptions (Tab. \ref{table:hs_input}). Then all descriptions was parsed to trees with three different approaches, described below.

\begin{table}
\begin{tabularx}{\textwidth}{ l X }
\hline
\textbf{Input query:} & while '<' is contained in value and '>' is contained in value, \\
\hline 
\textbf{Input query preprocessed:} & while \_STR\_0\_ is contained in value and \_STR\_1\_ is contained in value , \\
\hline 
\textbf{Target code:} & while '<' in value and '>' in value: \\
\hline 
\textbf{Target code preprocessed:} & while '\_STR\_0\_' in value and '\_STR\_1\_' in value: \\
\hline
\end{tabularx}
\caption[Quoted items preprocessing]{Quoted items preprocessing item \#2 from the developer split of \textbf{Django}.}
\label{table:str_markers}
\end{table}

\begin{table}
\begin{tabularx}{\textwidth}{ l X }
\hline
\textbf{Input query:} & from django.utils.six.moves import html\_parser as \_html\_parse into default name space. \\
\hline 
\textbf{Input query preprocessed:} & from django.utils.six.moves ( django utils six moves ) import html\_parser as \_html\_parse into default name space . \\
\hline 
\end{tabularx}
\caption[Nested object references preprocessing]{Nested bject references preprocessing for item \#93 from the developer split of \textbf{Django}.}
\label{table:function_calls}
\end{table}

\begin{table}
\begin{tabularx}{\textwidth}{ l X }
\hline
\textbf{Structured input:} & Deadly Poison NAME\_END -1 ATK\_END -1 DEF\_END 1 COST\_END -1 DUR\_END Spell TYPE\_END Rogue PLAYER\_CLS\_END NIL RACE\_END Free RARITY\_END Give your weapon +2 Attack. \\
\hline 
\textbf{Synthetic description:} & Name: Deadly Poison, attack: -1, defence: -1, cost: 1, duration: -1, type: Spell, player class: Rogue, race: None, rarity: Free. Give your weapon +2 Attack. \\
\hline
\end{tabularx}
\caption[Synthetic description example]{Synthetic description for the item \#3 from the developer split of \textbf{HS}.}
\label{table:hs_input}
\end{table}

To create CFG sentence representation we used \code{LexicalizedParser} \parencite{klein2003} from the CoreNLP. Dependency parsing was done by \code{DependencyParser} \parencite{chen2014} from the CoreNLP. For CCG parsing we used package EasyCCG\footnote{\href{http://homepages.inf.ed.ac.uk/s1049478/easyccg.html}{http://homepages.inf.ed.ac.uk/s1049478/easyccg.html}} \parencite{lewis2014}.\footnote{We were not able to include figures with examples parse trees in the thesis due to their large size, but you can find them at our \href{https://github.com/tsdaemon/treelstm-code-generation/tree/master/pictures}{GitHub repo}}

\section{Implementation details}

\textbf{Dynamic computational graph.} Model of \cite{Yin2017} was build on framework Theano\footnote{\href{http://deeplearning.net/software/theano/}{http://deeplearning.net/software/theano/}}. But Theano is not able to build dynamic computational graph to encode syntactic trees. Therefore, we implemented our model on PyTorch\footnote{\href{http://pytorch.org/}{http://pytorch.org/}}.

\textbf{Model parameters.} The size of all embeddings is 256, except for word embeddings, which is 300. For word embeddings we used preptrained Common Crawl GloVe vectors \parencite{pennington2014}. We have not freeze weights of word embeddings, so  pretrained values can be additionaly adjusted during training. The dimensions of encoder and decoder hidden states and memory cells are 256. Hidden states of attention and pointer networks are of size 50. Also, we used last state of encoder as initial state of decoder (thought vector). For decoding we used beam size 10.

% https://r2rt.com/non-zero-initial-states-for-recurrent-neural-networks.html
\textbf{Regularization.} Since our datasets are relatively small for  a such complex neural model, we added strong regularization using Variational Dropout suggested in work of \cite{Gal2016}. Similary to approach described in work of \cite{zimmermann2012} we added Gaussian noise with mean 0.0 and STD 0.1 to initial states $h^{(0)}$ and $c^{(0)}$ of encoder. These methods added statistically significant improvement of both training speed and validation scores.

\section{Experimental setup} \label{exp_setup}

\textbf{Evaluation metrics.} For this experiment we measured \textbf{accuracy} as a fraction of output code which fully match target examples. Additionally, to measure quality of examples without full match we used average token level \textbf{BLEU-4}, as suggested by \cite{Ling2016} and \cite{Yin2017}. However, BLEU and accuracy do not measure actual correctness of a generated code. Therefore we defined an \textbf{errors} metric as a fraction of output trees which we was not able to convert into code.

\textbf{Baseline.} Along with Tree-LSTM encoder we prepared model previously decribed in work of \cite{Yin2017}, with vanilla bidirectional LSTM encoder. This was done to have clear baseline for semantic encoding method. 

\section{Results}
Evaluation results are listed in Tab. \ref{table:evaluation}. The best model was selected using results of evaluation on development set and then evaluated on the separate test set. We compared with two approaches: (1) Latent Predictor Network (LPN) \parencite{Ling2016} and (2) Syntactic Neural Model \parencite{Yin2017}. 
\textbf{Results analysis.} As presented in Tab. \ref{table:evaluation}, performance of models with Tree-LSTM encoder have results comparable with LPN. Yet no model was able to improve current state-of-the-art results of Syntactic Neural Model. 

Although all type of query trees have almost equal performance, dependency trees encoding have shown better results for HS. Dependency trees do not contain phrasal node and therefore shorter. This can be a proper justification for the difference.

\begin{table}
\begin{tabular}[h]{ l c c c c c c }

\hline
& \multicolumn{3}{c}{\textbf{HS}} & \multicolumn{3}{c}{\textbf{Django}}\\
\hline
& ACC & BLEU & ERROR & ACC & BLEU & ERROR \\
\hline
Retrieval system:$^\dagger$ & 0.0 & 62.5 & - & 14.7 & 18.6 & - \\
Phrasal statistical MT:$^\dagger$ & 0.0 & 34.1 & - & 31.5 & 47.6 & - \\
Hierarchical statistical MT:$^\dagger$ & 0.0 & 43.2 & - & 9.5 & 35.9 & - \\
\hline 
NMT$^\ddagger$ & 1.5 & 60.4 & - & 45.1 & 63.4 & - \\
Seq2Tree$^\ddagger$ & 1.5 & 53.4 & - & 28.9 & 44.6 & - \\
Seq2Tree-UNK$^\ddagger$ & 13.6 & 62.8 & - & 39.4 & 58.2 & - \\
LPN$^\dagger$ & 4.5 & 65.6 & - & 62.3 & 77.6 & - \\
Syntactic Neural Model$^\ddagger$ & \textbf{16.7} & \textbf{75.8} & - & \textbf{71.6} & \textbf{84.5} & - \\
\hline
\multicolumn{7}{l}{\textbf{w/ unary closures:}}\\
Bi-directional LSTM encoder & 9.1 & 71.6 & 0.6 &  \\
\multicolumn{7}{l}{Tree-LSTM encoder} \\
\ with dependency trees & 4.5 & 66.7 & 3.0 &  \\
\ with constituency trees & 4.5 & 63.9 & 13.2 &  \\
\ with CCG trees & 3.0 & 66.1 & 5.1 &  \\
\multicolumn{7}{l}{\textbf{w/o unary closures:}}\\
Bi-directional LSTM encoder & 16.2 & 69.2 & 0.3 &  \\
\multicolumn{7}{l}{Tree-LSTM encoder} \\
\ with dependency trees & 6 & 71.5 & 4.0 &  \\
\ with constituency trees & 4.5 & 64.9 & 13.2 &  \\
\ with CCG trees & 3.0 & 65.3 & 11.0 &  \\
\hline
\end{tabular}
\caption[Final results]{Evaluation results for both datasets. $^\dagger$Results previously reported by \cite{Ling2016}. $^\ddagger$Results previously reported by \cite{Yin2017}.}
\label{table:evaluation}
\end{table}




