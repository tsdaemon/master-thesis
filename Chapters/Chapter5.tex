\chapter{Experiments} \label{Chapter5} 


\section{Datasets preprocessing} \label{preprocessing}
% A* CCG Parsing with a Supertag and Dependency Factored Model
% https://nlp.stanford.edu/pubs/glove.pdf 
% dependency parsing https://web.stanford.edu/~jurafsky/slp3/
% https://nlp.stanford.edu/software/stanford-dependencies.shtml
% CCG - mark steedman
% Danqi Chen and Christopher D Manning. 2014. A Fast and Accurate Dependency Parser using Neural Networks. Proceedings of EMNLP 2014
% Jonathan Berant, Andrew Chou, Roy Frostig, Percy Liang. Semantic Parsing on Freebase from Question-Answer Pairs. Empirical Methods in Natural Language Processing (EMNLP), 2013.

\section{Evaluation metrics}
BLEU
Accuracy
Error

rewrite: These two metrics are not ideal: accuracy only measures exact match and thus lacks the ability to give credit to semantically correct code that is different from the reference, while it is not clear whether BLEU provides an appropriate proxy for measuring semantics in the code generation task. A more
intriguing metric would be directly measuring semantic/functional code equivalence, for which we present a pilot study
at the end of this section (cf. Error Analysis). We leave exploring more sophisticated metrics (e.g. based on static code
analysis) as future work.

\section{Results}

Describe previous results. Could be used results from \cite{Yin2017} and \cite{Barone2017}.


