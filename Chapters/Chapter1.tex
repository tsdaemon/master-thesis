% Chapter 1

\chapter{Introduction} % Main chapter title

\label{Chapter1} % For referencing the chapter elsewhere, use \ref{Chapter1} 

%----------------------------------------------------------------------------------------

% Define some commands to keep the formatting separated from the content 
\newcommand{\keyword}[1]{\textbf{#1}}
\newcommand{\tabhead}[1]{\textbf{#1}}

%----------------------------------------------------------------------------------------

\section{Motivation}
Software development described as a knowledge-intensive field \parencite{Robillard1999}. Implementation and maintenance of enterprise software system require broad knowledge of different programming languages and application programming interfaces. While in 2002 to create a website developer had to know HTML/CSS, PHP and MySQL, in 2017 it requires knowledge about frontend ecosystem, backend frameworks, and different NoSQL query languages. Documentation becomes a bottleneck while solving simple tasks, especially for new developers. Though software development involves regular use of search engines and Q\&A databases \parencite{Treude2011}. Code snippets from crowdsourced resources like StackOverflow usually adopted and reused in other projects. Developers often seek to find existing examples of working code to solve regular tasks instead of write and test it from scratch \parencite{Brandt2010}. And to find corresponding code snippet software developer first formulates its description as a query for search engine \parencite{Brandt2009}. 

However, a web search is time-consuming and cause interruptions of the coding process. As an alternative, this description could be translated directly to code. Such translation tool would reduce the burden of remembering the details of a particular language or API and allow a developer to use his time for more creative aspects of development. I want to create dynamic, interactive and easy-to-use code generation tool which would allow to translate code description to actual implementation. 

% It uses a code description to generate a required snippet of code just under the programmer cursor.

%----------------------------------------------------------------------------------------

\section{Goals}

\begin{enumerate}
	\item Explore previous examples of code generation or code search tools.
	\item Train Description2Code syntactic model and compare its performance with previous results.
	\item Develop code generation plugin for PyCharm IDE.
\end{enumerate}

%----------------------------------------------------------------------------------------

\section{Related works}

\subsection{Automatic programming}

A problem of translation of high-order specifications to low-order instructions was recognized at the early years of computational industry. It addresses the main goal of computer science and artificial general intelligence - to shift the burden of requirements understanding and instructions implementation from human to machine. The notion of \emph{automatic} programming was first established in FORTRAN compiler in 1957 \parencite{backus1957fortran} and used as a prototype of high-order programming concept.  Later automatic programming splits into two complementary approaches: bottom-up and top-down \parencite{Balzer1985}. In the first approach, a specification language is developed as a set of high order functions and modules. And in the second approach, informal specification language is adopted to the formal level, which can be compiled automatically. While the first approach was a background for high-order programming languages and frameworks, the second approach gave a raise for automatic code generation field. 

First attempts to build automatic programming system addressed the roles of symbolic evaluations, deduction and programming knowledge in the programming process. That was coherent with symbolic artificial intelligence, a dominant paradigm in artificial intelligence research from the mid-1950s until the late 1980s \parencite{haugeland1989artificial}. Works of \cite{green1969application} and \cite{Lee1974} was focused on the use of theorem-prover to produce the programs. In 1976, the PSI program synthesis system \parencite{green1976design, green1977summary} was concerned with coding a high-level program knowledge from requirements collected via dialog with a user. It used a set of expert modules to build program model from natural language and generate a code for this model. For example, one of the generator modules was PECOS \parencite{barstow1979experiment}, which used a set of symbolic rules to design abstract algorithms like sorting or graph reaching. 

Another automated coding attempt was made in 1978 with project SAFE \parencite{balzer1978informality}. It is used semantic parsing to resolve ambiguity in informal specifications and translate them into a symbolic representation. While SAFE was a laboratory prototype designed to solve a limited set of tasks, its results were used in further automated programing researches like specification language Gist \parencite{Balzer1985} in 1985 and automatic requirement derivation system SPECIFIER in 1991 \parencite{Miriyala1991}.

Although these works have given a great advance in ideas about knowledge representation and informality translation, they have major flaws. Symbolic approach to artificial intelligence was criticized \parencite{mcdermott1987critique, harnad1990symbol} for symbolic grounding problem and problems with uncertainty representation. \cite{dreyfus1994computers} argued that symbols and formal rules could not catch unconscious instincts which form human intelligence. Therefore latter research of automatic programming addressed this problem with statistical machine learning methods like neural networks and representation learning.

\subsection{Deep learning} 
\emph{Deep neural networks} have two major advances for the natural language modeling and translation. The first is \emph{representation learning} \parencite{Bengio2013}, which allows to transform data into the representation which contains important features for current task. This feature allows to create knowledge representations of informal instructions automatically, without complex preparations. And the second is ability to approximate statistical distribution prior to some conditions \parencite{white1992artificial}, which allow to deal with uncertainty in language interpretation. 

With invention of word embeddings \parencite{bengio2003neural} sequences of informal instructions became a possible input for neural models. Great advance in language modeling was introduced by \emph{recurrent neural networks} \parencite{sundermeyer2012lstm, hochreiter1997long, Jozefowicz2016, Gers2001} which able to capture features encoded in sequential structure of natural language. \emph{Sequence to sequence models} \parencite{NIPS2014_5346} with recent novel techniques like \emph{attention technique} \parencite{Luong2015, Jean2014, Bahdanau2014}, which allowed to surpass results of phrase-based machine translation in real scale system like Google Translation \parencite{Wu2016}.

Code generation with neural models was interpreted as a neural machine translation problem and used established approach: Sequence2Sequence with attention. \cite{Zhong2017} uses pointer based generation and reinforcement learning for generation of SQL queries from informal questions. \cite{Ling2016} proposed an architecture of latent predictor networks with structured attention for generation of Magic: The Gathering and HearthStone cards implementation from card description in Java and Python. In \cite{Chen2016} used latent attention to generate If-Then recipes for natural descriptions for IFTTT.com dataset.

\subsection{Snippet generation}

Instead of end-to-end translation of complex high-order requirements to software system into compiled instructions, code generation could be used as a handy \emph{snippet generation} tool. For example, \cite{little2009keyword} proposed a keyword based generation of Java code and implemented it in integrated development environment plugin for Eclipse. \cite{Gvero2015} addresses the problem of description to code generation as a machine translation task and use probabilistic context-free grammar model to generate a list of ranked expressions for a developer. Codehint \parencite{Galenson2014}, another plugin for Eclipse, explores Java virtual machine execution state to build search space of possible expressions for given description and then select the most likely one using the statistical model built offline from existing projects. NaturalJava \parencite{Price2000} accepts English sentences as input and uses decision trees to infer Java abstract syntax tree.

Another common approach in code generation is a creation of code for predefined context, in other words, input and output of a module. These solutions allow to fill gaps and create connections in the program template. \cite{Raychev2014} used statistical language models to synthesize the code for holes in application programming interfaces with a most likely sequence of statements. \cite{Jha2010} proposes code generation approach with I/O oracle, constrained to with input-output behavior and a set of available components. Search in the expression space performed with off-the-shelf Satisfiability Modulo Theory solvers. Domain-specific solution StreamBit \parencite{Solar-Lezama2005} shifts the task of most complex and error sensitive bit-level operations from programmer to a code generator. A developer only needs to write a sketch - a domain-specific description which then translated to C implementation. \cite{Srivastava2010} used verification tools to perform proof-theoretic program synthesis with given an input-output functional specification and a specification of the synthesized programâ€™s looping structure.

\subsection{Semantic language models}
A semantic model of the natural language is a meaning representation which could be understood by the machine. Tasks like natural language understanding, paraphrase detection heavily relies on semantic models like abstract meaning representation \parencite{banarescu2013abstract} or combinatory categorial grammar \parencite{Clark2007}. This models also could be domain specific formal representation like natural language database interface \parencite{Zettlemoyer2012, berant2013semantic}, instructions for robot \parencite{artzi2013weakly} or smart home instructions \parencite{quirk2015language}. Since semantic meaning representation already contains formal instructions, their use for code generation task could significantly improve the model. 

Previous years in the neural modeling of natural language dominated an idea that neural networks do not require any information about a syntactic or semantic structure of sentences. Outstanding results of deep neural networks in representation learning and structure parsing suggested to use a plain sequence of words as an input for a neural model and allow it to learn a representation of all other important features backpropagating the gradient of error. However, recent results of syntactic structures usage in machine translation \parencite{Chen2017} and reading comprehension \parencite{xie2017constituent} outperform the result of Sequence2Sequence models. To parse syntactic structures was used \emph{recursive neural networks} \parencite{Gollera, socher2011parsing} and \emph{recurrent neural networks grammar} \parencite{Dyer2016}. Recursive neural networks also have shown good results for semantic \parencite{Tai2015} and dependency parsing \parencite{Zhu2015}. In the field of code generation, this approach addressed the problem of code syntax model usage for generation of the valid program. \cite{Yin2017} and \cite{Rabinovich2017} used a structured decoder to generate abstract syntax tree instead of a sequence of code symbols. Thus model search space was significantly reduced and prior knowledge about language syntax was used for valid code generation.

\subsection{Datasets}
\emph{HearthStone} (HS) dataset \parencite{Ling2016} is a collection of Python classes which implements cards from the card game HearthStone. Each card has a set of attributes which is concatenated to produce the input sequence. The dataset contains 665 Python classes with descriptions.

\emph{Django} dataset \parencite{Oda2015} contains a corpus of lines of Python code with manually annotated pseudocode from the Django web framework. Corpus contains 18,805 pairs of Python statements and corresponding English pseudo-code. 

\cite{Barone2017} (BS) created the dataset of Python code with parallel descriptions from GitHub. It contains 150,370 triples of function declarations, docstrings and bodies. This dataset presents the most challenging task because it contains highly heterogeneous and noisy data. Django pseudo-code is already has a good alignment with target code and HeartStone code examples are mostly homogeneous and target specific domain.
%----------------------------------------------------------------------------------------

\section{Comparison with other works}

Keyword programming approach \parencite{little2009keyword} is tailored to Java syntax and requires substantial work to be reused in other languages. The same problem has plugin anyCode, described in \cite{Gvero2015}. Plugin Codehint \parencite{Galenson2014} implemented handy user experience for code generation. It uses specially marked comments as a place marker to insert generated code. However, its approach of usage Java virtual machine state as exploration space could not be transferred to dynamic typed languages like Python. Model, described in this work do not require to run program to generate target code. And, it is language agnostic and could be trained for any language requiring only appropriate dataset of code lines and descriptions.

Architecture described in \cite{Zhong2017} shows interesting approach with pointer networks for reuse of description parts, for example variables or function names. However, it is domain specific as natural language database interface, thus it can not be compared with general use code generation. \cite{Chen2016} and \cite{Ling2016} also described only domain specific code generation.

\cite{Yin2017} proposed to use vanilla LSTM encoder and augmented LSTM with additional neural connections to reflect the structure of abstract syntax tree. This idea shows have great potential as it infers code model in its structure. However, it is improved in my work by using Tree LSTM as encoder with parsed semantic tree as input.  Additionally, datasets Django and HearthStone used to evaluate \cite{Yin2017} project have few major flaws. Django do not actually contains natural descriptions of code, it is composed with parallel pseudo-code for each line of code, so it is already have a good alignment with target code. HearthStone contains Python implementation of game cards, though this code is domain specific and homogeneous. My project is evaluated on \cite{Barone2017} dataset of Python code descriptions. This dataset is mined from comments to code and functions of open source projects at GitHub, though it is highly heterogeneous and truly reflect the problem of general description to code translation.

