\chapter{Related works}
\label{Chapter2}

\section{Automatic programming}
A problem of translation of high-level specifications to low-level instructions was recognized at the early years of computational industry. It addresses the important goal of computer science and artificial general intelligence --- to shift the burden of requirements understanding and instructions implementation from human to machine. The notion of \emph{automatic programming} was first established in FORTRAN compiler in 1957 \parencite{backus1957fortran} and used as a prototype of high-level programming concept.  Later automatic programming split into two complementary approaches: bottom-up and top-down \parencite{Balzer1985}. In the first approach, a specification language is developed as a set of high-level functions and modules. And in the second approach, informal specification language is translated to the formal level, which can be compiled automatically. While the first approach was a background for high-order programming languages and frameworks, the second approach gave raise to automatic code generation field. 

First attempts to build automatic programming system addressed the roles of symbolic evaluations, deduction and programming knowledge in the programming process. That was coherent with symbolic artificial intelligence, a dominant paradigm in artificial intelligence research from the mid-1950s until the late 1980s \parencite{haugeland1989artificial}. \cite{green1969application} and \cite{Lee1974} was focused on the use of theorem-prover to produce the programs. In 1976, the PSI program synthesis system \parencite{green1976design, green1977summary} was concerned with coding a high-level program knowledge from requirements collected via dialog with a user. It used a set of expert modules to build program model from natural language and generate a code for this model. For example, one of the generator modules was PECOS \parencite{barstow1979experiment}, which used a set of symbolic rules to design abstract algorithms like sorting or path finding. 

Another automated coding attempt was made in 1978 with project SAFE \parencite{balzer1978informality}. It used semantic parsing to resolve ambiguity in informal specifications and translated them into a symbolic representation. While SAFE was a laboratory prototype designed to solve a limited set of tasks, its results were used in further automated programing researches like specification language Gist \parencite{Balzer1985} in 1985 and automatic requirement derivation system SPECIFIER in 1991 \parencite{Miriyala1991}.

Although these works have given a great advance in ideas about knowledge representation and informality translation, they have major flaws. Symbolic approach to artificial intelligence was criticized \parencite{mcdermott1987critique, harnad1990symbol} for symbolic grounding problem and problems with uncertainty representation. \cite{dreyfus1994computers} argued that symbols and formal rules could not catch unconscious instincts which form human intelligence. Therefore further research of automatic programming addressed this problem with statistical machine learning methods like neural networks and representation learning.

\section{Deep learning} 
\emph{Deep neural networks} have two major advantages for the natural language modeling and translation. The first is \emph{representation learning} \parencite{Bengio2013}, which allows to transform data into the representation which contains important features for current task. This feature allows to create knowledge representations of informal instructions automatically, without complex preparations. And the second is the ability to approximate statistical distribution prior to some conditions \parencite{white1992artificial}, which allows to deal with uncertainty in the language interpretation. 

With invention of word embeddings \parencite{bengio2003neural} sequences of informal instructions became a possible input for neural models. Great advance in language modeling was introduced by \emph{recurrent neural networks} \parencite{sundermeyer2012lstm, hochreiter1997long, Jozefowicz2016, Gers2001} which were able to capture features encoded in sequential structure of natural language. \emph{Sequence-to-sequence models} \parencite{NIPS2014_5346} with recent novel methods like \emph{attention technique} \parencite{Luong2015, Jean2014, Bahdanau2014} allowed to surpass results of phrase-based machine translation in production level system like Google Translation \parencite{Wu2016}. 

Code generation with neural models was interpreted as a neural machine translation problem and used established approach --- sequence-to-sequence model with attention. \cite{Ling2016} proposed an architecture of latent predictor networks with structured attention for generation of Magic: The Gathering and HearthStone cards implementation from card description in Java and Python. \cite{Chen2016} used latent attention to generate If-Then recipes for natural descriptions for IFTTT.com dataset. Remarkable idea of pointer networks introduced by \cite{NIPS2015_5866} allow to re-use parts of input sequence in the output. It was used by \cite{Zhong2017} along with reinforcement learning for generation of SQL queries from informal questions \parencite{Bhoopchand2016}. 

\section{Snippet generation}
Instead of end-to-end translation of complex high-order requirements to software system into compiled instructions, code generation could be used as a handy \emph{snippet generation} tool. For example, \cite{little2009keyword} proposed a keyword based generation of Java code and implemented it in integrated development environment plugin for Eclipse. \cite{Gvero2015} addressed the problem of description-to-code generation as a machine translation task and used probabilistic context-free grammar model to generate a list of ranked code expressions for a developer. Codehint \parencite{Galenson2014}, another plugin for Eclipse, explored Java virtual machine execution state to build search space of possible expressions for given description and then to select the most likely one using the statistical model built offline from existing projects. NaturalJava \parencite{Price2000} uses decision trees to infer Java abstract syntax tree from English sentences. \cite{pmlr-v37-allamanis15} created bimodal models of language and code to generate code from description and reversed for C\#.

Another common approach in code generation is a creation of code for predefined context, in other words, input and output of a module. These solutions allow to fill gaps and create connections in the program template. \cite{Raychev2014} used statistical language models to synthesize the code for holes in application programming interfaces with a most likely sequence of statements. \cite{Jha2010} propose code generation approach with I/O oracle, constrained to input-output behavior and a set of available components. Search in the expression space performed with off-the-shelf Satisfiability Modulo Theory solvers. Domain-specific solution StreamBit \parencite{Solar-Lezama2005} shifts the task of most complex and error sensitive bit-level operations from programmer to a code generator. A developer only needs to write a sketch --- a domain-specific description which then gets translated to C implementation. \cite{Srivastava2010} used verification tools to perform proof-theoretic program synthesis with a given input-output functional specification and a specification of the synthesized programâ€™s looping structure.

\section{Semantic language models}
A semantic model of the natural language is a meaning representation which could be understood by the machine. Tasks like natural language understanding or paraphrase detection heavily relies on semantic models like abstract meaning representation \parencite{banarescu2013abstract} or combinatory categorial grammar \parencite{Clark2007}. These models also could be domain specific formal representation like natural language database interface \parencite{Zettlemoyer2012, berant2013semantic}, instructions for robot \parencite{artzi2013weakly} or smart home instructions \parencite{quirk2015language}. Since semantic meaning representation already contains formal instructions, their use for code generation task could significantly improve the model. 

Until recently in the neural modeling of natural language dominated an idea that neural networks do not require any information about a syntactic or semantic structure of sentences. Outstanding results of deep neural networks in representation learning and structure parsing suggested to use a plain sequence of words as an input for a neural model and allow it to learn a representation of all other important features backpropagating the gradient of error. However, recent results of syntactic structures usage for machine translation \parencite{Chen2017} and reading comprehension \parencite{xie2017constituent} outperform the result of Seq2Seq models. To parse syntactic structures, \emph{recursive neural networks} \parencite{Goller, socher2011parsing} and \emph{recurrent neural networks grammar} \parencite{Dyer2016} were used. Recursive neural networks also have shown good results for semantic \parencite{Tai2015} and dependency parsing \parencite{Zhu2015}. 

Another approach of syntax structure usage was addressed by \cite{Dong2016, Yin2017, Rabinovich2017}. In these works augmented decoder was used to generate a tree (syntax tree for syntactic parsing or abstract syntax tree for code generation) instead of a sequence. These models infer output structure and model search space was significantly reduced and prior knowledge about language syntax was used for valid code generation.

\section{Comparison with other works}

Keyword programming approach \parencite{little2009keyword} is tailored to Java syntax and requires substantial work to be reused in other languages. The same problem has plugin anyCode, described in \cite{Gvero2015}. Plugin Codehint \parencite{Galenson2014} implemented handy user experience for code generation. It uses specially marked comments as a place marker to insert generated code. However, its approach of usage Java virtual machine state as exploration space could not be transferred to dynamic typed languages like Python. The model described in our work does` not require to run program to generate target code. And, it is language agnostic and could be trained for any language requiring only appropriate dataset of code lines and descriptions.

Architecture described in \cite{Zhong2017} shows interesting approach with pointer networks for reuse of description parts, for example variables or function names. However, it is domain specific as natural language database interface, thus it can not be compared with general use code generation. \cite{Chen2016} and \cite{Ling2016} also described only domain specific code generation.

\cite{Yin2017} proposed to use vanilla long short-term memory encoder and augmented long short-term memory decoder with additional neural connections to reflect the structure of abstract syntax tree. This idea has great potential as it infers code model in its structure. In our work the above approach is modified by using tree long short-term memory as encoder with parsed semantic trees as input.

%essentially improved%
%Another difference between the present work and \cite{Yin2017} lies in evaluation process. Django and HearthStone used to evaluate \cite{Yin2017} have few major flaws. Django do not actually contains natural descriptions of code, it is composed with parallel pseudo-code for each line of code, so it is already have a good alignment with target code. HearthStone contains Python implementation of game cards, though this code is domain specific and homogeneous. My project is evaluated on \cite{Barone2017} dataset of Python code descriptions. This dataset is mined from comments to code and functions of open source projects at GitHub, though it is highly heterogeneous and truly reflect the problem of general description to code translation.