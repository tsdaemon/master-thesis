\chapter{Conclusion} 

\label{Chapter6} 

In this work, we have not reached a substantial performance improvement over the previously reported results. Conventional approach with BiLSTM encoder has shown better results with the lower error rate. What also has a significant value, it trains faster and it can be trained using batches. However, our study of the Tree2Tree models has an important value for the further exploration of machine translation and code generation. We believe that potential of Tree-LSTM is yet to be discovered in other applications and we want to continue our research in this field.

\section{Contribution}
In this work we made the following contribution:
\begin{itemize}
    \item Implemented Tree2Tree model on PyTorch.\footnote{All code is available on \href{https://github.com/tsdaemon/treelstm-code-generation/}{GitHub}.}
    \item Evaluated performance of tree encoders in sequence-to-sequence model.
    \item Created online API for code generation. \footnote{API is available \href{http://daemon-engineer.com/apps/codegen}{here}.}
\end{itemize}

\section{Points to improve} \label{improve}
\textbf{Tree encoder.} The recursive encoder has not surpassed results of the conventional BiLSTM encoder. However, it still has a potential to explore. Embeddings for lexical categories can be added to an input along with the word embeddings. Additional layers (recursive or recurrent) can be added on the top of the first layer. Knowledge about a tree structure can be induced into the attention layer to make an attention coherent with a query hierarchy. But since the syntactic models require a substantial effort to maintain the query parsing and cannot be encoded in batches, we do not consider this as a priority for the code generation task.

\textbf{Improve grammar model.} As we mentioned in \cref{ast_gen}, the grammar model that we have used in this project is not suitable for a code with a long sequences of expressions in its AST. This can be solved with another grammar model, which supports arbitrary number of children nodes for the list nodes. A special final action can be used to indicate the list end. 

\textbf{Code context encoding.} Our model used as an input only code description. Obviously, other code around a code line can contain important information which can be used for generation. In this model, we used an unconstrained terminal vocabulary, what is the naive approach, since each code line has different names of variables and functions in its scope. Therefore, usage of a code context in the model can made significant improvement for the correctness of the result.

\textbf{Datasets.} Datasets used for this work has few important flaws. \textbf{HS} is homogeneous and small, therefore it can only be used for a model evaluation and experiments. \textbf{Django} actually does not contains NL descriptions since it has pseudo-codes generated from the underlying code. Therefore this model requires evaluation on more heterogeneous datasets (like created by \cite{Barone2017}). And the initial goal of development of an IDE plugin capable to be a handy tool for any developer, requires dataset which was created with a special attention to the most frequent developer requests. Probably, StackOverflow can be used as a source of statistics for that.

\textbf{Evaluation.} As mentioned in \cref{exp_setup}, BLEU is a metric specifically designed for a human languages translation evaluation, therefore it can not appropriately represent the performance of a language to code translation. Tools like unit testing or static code analysis can provide measurements more relevant to this domain field. This should be considered during a development of more appropriate language-to-code dataset.

\textbf{Another applications.} Developed in this project codebase can be easily reused. The idea of the structured decoding has a great potential in other problems like question answering or text generation. We are planning to continue our research of other problems on a base of this work.
